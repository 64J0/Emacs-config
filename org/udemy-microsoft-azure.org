#+TITLE: [Udemy] Microsoft Azure: Do Zero a Certificação 2021 + 3 cursos
#+AUTHOR: Vinícius Gajo Marques Oliveira
#+EMAIL: vinigaio97@gmail.com

* Storage accounts
  When you create  a storage account general purpose version  2 you could create
  several storages within this account. For instance the allowed types are:

  - Blob containers
    Built to store big quantity of data, video streaming niche.
  - File Shares
    Business applications.
  - Queues
  - Tables

** Manage storage
  We could use Azure Storage Explorer application to manage our storage in Azure
  easily.     The     link     to     download    this     tool     is     this:
  https://azure.microsoft.com/en-us/features/storage-explorer/.

** Permissions and SAS
   When creating a new storage in the  storage account we can specify the Public
   access level. The default is private.
   
   Access Policy
   - General rules.

   SAS (Share Access Signature)
   - Specific rules.
     Specify permissions: RWDLAC.
       - R: Read
       - W: Write
       - D: Delete
       - L: List
       - A: Add
       - C: Create
     Specify operation time.

** Storage CLI
   The instructor is using PowerShell/CloudShell, running as administrator.

   It's necessary  to inform the  resource group, name, location,  kind (storage
   v2), tier (hot or cold), skuname (replication - Standard_LRS).

   Since I'm not  planning to use the Windows enviroment  I'll need to translate
   the commands to the Linux ecosystem.

#+begin_src bash
  $ az storage account create --name <NAME> --resource-group <RESOURCE_GROUP> --location <LOCATION> --sku <SKU> --kind <KIND>
#+end_src

** AzCopy
   CLI  tool provided  by  Azure  to handle  files  from  Azure. The  instructor
   presented the official docs, and he is using the version 10 of this tool.

   The action done in this class was the following:
   + Instructor creates a folder and put an image there (created with Paint)
   + Using the AzCopy tool, the instructor just send the image to the Azure storage
   + Instructor used PowerShell to run the commands
   
#+begin_src bash
  $ ./azcopy.exe copy "origin" "destiny" --recursive
#+end_src

   Since  the storage  container was  private,  instructor decided  to create  a
   shared access signature to enable the communication with his terminal.

   To make a download to the local environment we just need to switch the origin
   and the destiny in the command.

** Custom Domain
   + It's required to have a domain (bought $).

   Instructor will use a CNAME to redirect the requests to a cool link.

   There are two ways to change the domain.  There is one way that we don't have
   downtime and there  is a way that  we have downtime, that  normally take some
   minutes/hours but  could also  take 48  hours. The  instructor is  taking the
   first approach  and there  he is  creating a CNAME  in AWS  that is  where he
   bought the domain.

   + TTL used: 60 seconds

** The import/export service
   We use this tool in several situations:
   + Data migration
     Example: uploading all data to cloud.
   + Content distribution
     Example: share data from matrix to filials.
   + Backup and archive
   + Recovery

   -> We could use the platform UI (requires high end internet connection).
   -> CLI tool
      Prepare the access disks (HDD or SSD) and send to some location.

   [[https://azure.microsoft.com/en-gb/services/storage/import-export/][Import/Export Azure docs]]
      
** Static site
   Using a blob storage  we can host a website and Azure  will provide an unique
   URL.

   When the instructor started creating the configuration for the static website
   inside the storage  account, azure automatically created a  folder inside the
   blob storage with the name web/. This  folder is where the instructor put the
   files to serve the client.

* Virtual Machine (VM)
  Back in old days a company need  to buy the hardware.  Inside this hardware we
  run   an  operational   system,  that   could   be  Windows,   Linux  or   Mac
  sometimes. Then, on top of the OS we install the applications.

  The problem with  this structure is that  we are limited to  one hardware with
  one OS  only. With virtualization we  still have access to  a single hardware,
  composed of CPU, memory and disk. On top of this hardware we have a hypervisor
  where we could install  several OS in the same machine. Then,  on top of those
  different OS we can install their applications.

  On Azure we can create those virtual machine really fast, in some minutes.

  [[https://docs.microsoft.com/en-us/azure/architecture/guide/technology-choices/compute-decision-tree][Azure docs - Choose an Azure compute service for your application]]

** VM tier
   Tier in this context is related to the VM classification according to Azure.

   General rules:
   + Tier A:
       Very basic machine.  Usually used for testing and  development.  They are
     the cheaper machines available.
   + Tier B:
       Still basic machine but this kind is burstable (you can use a boost for a
     small amount of time).
   + Tier D:
       General  purpose  applications. Give  us  the  possibility to  have  more
     storage.
   + Tier E:
       Memory optimized machines.
   + Tier F:
       CPU optimized machines. More cores and more processing.
   + Tier G:
       Godzilla. Giant machines.
   + Tier H:
       High performance compute. Scientists use this kind of machine.
   + Tier L:
       High I/O storage.
   + Tier M:
       Large memory - TB RAM.
   + Tier N:
       GPU intensive computations.
   + Tier SAP-HANA:
       Specific instance for Azure certification.

   Before creating the virtual machine we can have an estimation of it's cost in
   the Azure platform.

** Azure Compute Units (ACU's)
   Created  by   Microsoft  and   it  describes  the   process  speed   in  some
   machine. There  is no  formula for  use to use  to completely  understand the
   relation for all machine tiers.

   As a general rule, when the value is bigger it can compute more things.

   Nested virtualization  -> could create  more virtualized machines  within the
   same machine.

** Windows 2019 machine
   When creating VM's using windows the price is a bit higher due to the license
   that  is paid.  If you  already have  a  license you  could use  it there  to
   decrease the costs.

   The strategy  adopted by the instructor  is to create a  resource group every
   time that  he is going to  start a new  laboratory experiment - focused  in a
   specific  theme, for  example the  projects  that deal  with virtual  machine
   experiments. This way it is much easier to delete everything that will not be
   necessary anymore.

   The default protocol to access Windows  machines is using RDP protocol in the
   port 3389.

   When  selecting the  disk,  the instructor  said that  he  always select  the
   Premium SSD.

** Acessing remotely through RDP
   + Instructor is using PowerShell to run the commands.

#+begin_src bash
  # get information about virtual machines
  $ get-azvm
  
  # access some vm
  $ Get-AzRemoteDesktopFile -ResourceGroupName <RG> -Name <NAME> -Launch
  # then it opens a different application that asks for the password
#+end_src

   This command  is used  only to  check the virtual  machine state.   Then, the
   instructor go  back to Azure Portal  and download a RDP  connection file. The
   interface with the Azure machine is pretty cool.

** Create a Linux VM
   + Instructor will use Ubuntu.
   + He will not set username + password, will use a key.
   + He will use PuTTY.
     ssh username@public-ip

** Data redundancy
   + LRS -> Locally Redundant Storage
   + ZRS -> Zone (Region)  Redundant Storage
   + GRS -> Geo Redundant Storage
   + RA-GRS -> Readable Geo Redundant Storage
   + GZRS -> Geo-Zone Redundant Storage
   + RA-GZRS -> Readable Geo-Zone Redundant Storage

** Disk storage
   Instructor will  add a new  storage in the Z:/  path for the  Windows machine
   that he created before.

   + Managed disk.

   Always assignt the extra disks to the same region as the other disks.

** Disk caching
   + VHD
   Using Disk  caching you make the  VHD performance much better  because it use
   RAM.

   + OS Disk
     Disk where we can install OS -> Win/Linux.
     Cache is Read-Write.
   + Data Disk
     Cache is none.

** IP address (VM)
   + VNET

** Availability set
   Provider grants 99.9% of SLA. But sometimes it goes down. You can define some
   policies to grant that the system will keep operational even if some disaster
   occur.

   + Contingency plan.

   + Fault domain -> Different racks inside the datacenter.
     Disaster scenario.
   + Update domain ->
     If MS need to apply some update  patch then it will move the application to
     other rack and  after the update finishes MS will  get the application back
     to the original rack.

   After the  creation of  VM it is  not possible to  apply a  new configuration
   related to this topic.  So, we first need to create  the Availability set and
   then create the VMs.

** Scaling set (virtual machine scale set)
   You can define  a rule to auto-scale  the quantity of VM's, based  in que CPU
   utilization.

   + Scale out:
     Rule to increase the quantity of VMs.
   + Scale in:
     Rule to decrease the quantity of VMs.

   Tip to increase the CPU % on Linux:

#+begin_src bash
  $ yes > /dev/null &
#+end_src
