#+TITLE: [book] The art of immutable architecture
#+AUTHOR: Michael L. Perry

+ Notes about the book, DevSecOps coaching.

* Chapter 1 - Why Immutable Architecture
  If you're in QA,  it is your job to imagine all of  the possible scenarios and
  then replicate them  in the lab. If  you are in development, you  need to code
  for all  of the  various exceptions  and race  conditions. And  if you  are in
  architecture, you  are responsible  for cutting the  Gordian Knot  of possible
  failures and  mitigations. This is the  fragile process by which  we build the
  systems that run our society.

** The immutability solution
   Distributed  systems  are  hard  to  write,  test,  and  maintain.  They  are
   unreliable, unpredictable, and  insecure. The process by which  we build them
   is certain to  miss defects that will  adversely affect our users.  But it is
   not your  fault. As  long as  we depend  upon individuals  to find,  fix, and
   mitigate these problems, defects will be missed.

   On its  face, immutability  is a  simple concept. Write  down some  data, and
   ensure  that  it  never  changes.  It can  never  be  modified,  updated,  or
   deleted.  It is  indelible. Immutability  solves the  problem of  distributed
   systems for one simple  reason: every copy of an immutable  object is just as
   good  as any  other copy.  As long  as things  never change,  keeping distant
   copies in sync is a trivial problem.

** Begin a new journey
   What this book seeks  is to model the business domain  as one large immutable
   data structure.  It would be impossible  for a single machine  or database to
   house that  entire structure. Nor  would that be  desirable. And so  the book
   also seeks  to demonstrate how  to implement  subsets of that  data structure
   within  individual  databases,  programs,   and  machines.  These  components
   communicate through  well-crafted protocols that honor  the idiosyncrasies of
   distributed systems to evolve that immutable data structure over time.

** The fallacies of distributed computing
   + The network is reliable
   + Latency is zero
   + Bandwidth is infinite
   + The network is secure
   + Topology doesn't change
   + There is one administrator
   + Transport cost is zero
   + The network is homogeneous

   I can recall  on several occasions being suprised that  a program that worked
   flawlessly on /localhost/ failed quickly when deployed to a test environment.

** Changing assumptions
   The fallacies of distributed computing are  easy assumptions to make. We make
   them  because our  tools,  specifications, and  training have  led  us to  do
   so.   The  non-idempotent   POST  verb   is  a   valid  part   of  the   HTTP
   specification. Auto-incrementing IDs are a  valuable feature of most database
   management  systems. Almost  every tutorial  on application  development will
   teach a beginner  to use these capabilities.  The fact that by  doing so they
   are making an incorrect assumption does not even occur to them.

   The tools that we use and the  patterns that we follow today all evolved from
   a  time during  which  assumptions  of high  reliability,  zero latency,  and
   topological consistency  were not  fallacies. In-process procedure  calls are
   perfectly  reliable.  Sequential  program  statements  have  very  low,  very
   predictable latency  characteristics. And sequential  counters in a  for loop
   will never return to the top of  the function to find the code's topology had
   changed. It's when we evolve  these abstractions into RPCs, network requests,
   and auto-incremented IDs that problems arise. When we apply the languages and
   patterns of the past to the problems  of modern distributed systems, it is no
   wonder that programmers will make incorrect assumptions.

   All of  the fallacies of  distributed computing  stem from one  simple truth:
   distributed systems are built using tools  designed to run a single thread on
   a single computer.

** Structural sharing
   The fact  that we  intend for  data structures  to be  immutable opens  a new
   possibility. As we build new data structures, we can reuse existing pieces of
   old data structures. There  is no need to copy those  pieces, because we have
   already established  that they  will not  change. We  simply create  new data
   elements to represent the ones that have  "changed" and let them point to the
   ones that haven't.

   This is a  technique called /structural sharing/. It's  a common optimization
   for immutable data structures that is enabled by immutable data structures.

* Chapter 2 - Forms of Immutable Architecture
  There  are   consequences  to   designing  a   system  using   only  immutable
  records. Some of them are the advantages that we've already explored:

  + reliable communications
  + reduced blocking
  + increased autonomy
  + improved auditability

  Other consequences are less desirable.

  The trade-offs  requiring shift to immutability  have led to the  emergence of
  different architectural styles. In this chapter we will examine three of those
  styles: Event  Sourcing (ES), Asynchronous  Model View Update,  and Historical
  Modeling.   All  three share  the  idea  that  state evolves  from  historical
  records. Where they diverge is in the ordering of those records. The first two
  styles assume that records can be viewed in /sequence/. They expect to be able
  to enumerate records in order. The  third arises from the idea that historical
  records may be /partially/ ordered. It does not allow enumeration. Instead, it
  trades that capability away to achieve some valuable results.

  After this chapter, the  remainder of the book will focus  on the third style:
  Historical Modeling.

** Deriving state from history
   Definitions:

   + /objects/: things that change (mutable)
   + /records/: things that do not change (immutable)

** Projections
   Our  goal now  is to  use  immutable records  to model  mutable objects.  The
   records clearly are  not the objects themselves. That  would be insufficient,
   as  records  would not  allow  for  the  mutability  that objects  expect  to
   have.  Instead, the  records  must in  some way  represent  /changes/ to  the
   objects. *The immutable records /are/ the mutations of the objects*.

   To  achieve  this  goal,  we   will  treat  immutable  records  as  /observed
   state/. They represent things that we  actually saw and recorded. Objects, on
   the other  hand, are  /derived/ state. They  represent out  interpretation of
   those observations and can change as new observations are made.

** Projecting objects
   No matter what  you call it - formulas, dependent  variables, projections, or
   view models  - *derived state  is a deterministic transformation  of observed
   state*.  It  adds  no  information  to  the  system;  it  only  presents  the
   information that's already there in a  different way.  In mathematics, we say
   that it adds no degrees of freedom  to the system. In software, you might say
   that the view model  is backed by the model. The important  point is that the
   user gets  to change observed state  directly. They can only  see the results
   indirectly projected onto the derived state.

   In an immutable architecture, the  historical records are observed state. The
   user gets to create new records directly through their actions. Those records
   capture decisions that the user has made.

   The  objects,   on  the  other   hand,  are  merely  projections.   They  are
   ephemeral. The user does not get to set the state of an object. They can only
   see those objects change as a result  of new historical records. Every one of
   these architectures has their own way of calculating that projection.

** Event Sourcing (ES)
   Historical records are the observed  state of an immutable architecture. They
   represent past  decisions. You could  call these past decisions  "events" and
   demand that they are the sole source of truth. That is the origin of the term
   /event sourcing/ (ES).

   In  an event-sourced  application,  the  user interacts  (through  an UI  and
   possibly an  API) with  a domain  model.  The domain  model does  not respond
   immediately to the  request. Instead, it validates the  request and generates
   an event. The event is an immutable  record of the user's intent. It is named
   and  interpreted as  a past-tense  statement,  as in  "this thing  happened":
   OrderSubmitted, PlayerRegistered, and ResidentMoved,  for example. The naming
   convention  reflects the  truth  that  an event,  once  generated, cannot  be
   ignored. Its effect might just be different from what the user intended.

   The advantages  that ES provides over  a traditional object model  begin with
   the same ones that we've  already identified for all immutable architectures:
   increased scalability and  auditability. In addition, they  boast the ability
   to rebuild objects entirely from the stream of events. When a defect is fixed
   or a failure is added, the application can discard any cached versions of the
   domain  model and  reconstruct them  using the  new code.  It also  allows an
   event-sourced  application to  go back  in  time and  replay only  part of  a
   sequence, seeing an object as it appeared in the past. This provides the user
   of the application with a powerful ability to perform temporal analysis.

** Commutative and Idempotent Events
   We will  find that the  commutative and  idempotent properties are  useful in
   distributed systems. The commutative property allows us to apply an operation
   out of order  and get the same  result. The idempotent property  allows us to
   repeat the operation without further effect. Since Event Source is based on a
   sequence of operations, it is sensitive  to both order and duplication. It is
   up  to the  application  developer  to ensure  that  order  is preserved  and
   duplicates are prevented.

** Asynchronous Model View Update
   The Elm programming language takes the functional view of a history of events
   quite  literally.  This language  compiles  to  JavaScript  and runs  in  the
   browser. It  generates HTML  from a  source model.  A pure  function produces
   successive versions of the model as it handles messages. The pattern on which
   Elm is based is called /Model View Update/.

   Inspired by Elm,  Facebook created a suite of tools  that extend this pattern
   to the server.  The first of those  tools was React, a  front-end library for
   JavaScript  that  projects   a  model  into  HTML.  The  next   was  Flux,  a
   unidirectional data  flow application  design pattern.  This defined  the way
   that Facebook  designed web and mobile  apps. Redux was an  implementation of
   Flux  developed outside  of Facebook  by  Dan Abramov.  Dan was  subsequently
   brought into Facebook to continue work  on Redux, React, and the architecture
   in general. Finally, there is the  back-end architecture, only parts of which
   are currently open sourced, upon which Facebook develops their APIs.

   The Asynchronous  Model View Update architecture  optimistically interprets a
   series of actions. User actions  are validated on-device with the expectation
   that most  of them will succeed  on the server.  It is assumed that  no other
   actions will  intervene and that the  result of executing the  actions on the
   server will be the same as on  the client. When this optimistic assumption is
   found  to be  false, the  architecture simply  discards the  locally computed
   state and takes the server's version.

** Historical Modeling
   The immutable  architectures that  we just examined  both make  a distinction
   between immutable  historical records and  a mutable object model.  They also
   assume that historical  events occurred within a fully  ordered sequence. But
   neither  of these  assumptions  necessarily  follow from  the  idea of  using
   immutable  records  as the  source  of  truth. If  we  model  a system  as  a
   collection of related historical facts, we find that we can dispense with the
   mutable  object model  altogether and  that facts  don't necessarily  have to
   occur in a sequence.

   Let's begin  with a  slight change  in terminology.  Instead of  referring to
   historical records  as /events/ or /actions/,  let us call them  /facts/. The
   reason for  the name change  is that facts  obey a set  of rules that  do not
   necessarily apply  to events  in Event Sourcing,  or actions  in Asynchronous
   Model View Update. In particular, facts are partially ordered.

*** Partial Order
    The term "partial  order" comes from mathematics, and  is distinguished from
    the term "full order". Start with a  set of objects, be they numbers, words,
    science  papers,  data  structures,  what  have  you.  Define  a  comparison
    operation that  tells you wether  one object  comes before another.  We will
    typically use the  less than symbol (<) to represent  this operation. If for
    any pair of  objects in the set, we  can use < to put one  before the other,
    then the  set is fully  ordered. If we  can only do  that for /some/  of the
    pairs, then the set is partially ordered.

    Whether  we  are  talking about  a  total  order  or  a partial  order,  the
    comparison  operator   that  we  choose   must  have  a  couple   of  useful
    properties. First, it must be *transitive*:

#+begin_src latex
  a < b and b < c \Rightarrow a < c
#+end_src

    It must  also be  *non-reflexive*. That is  to say that  an object  does not
    "come before" itself.

#+begin_src latex
  a \nless a
#+end_src

    Finally, the comparison operation must be unidirectional. That means that an
    object cannot come both before and after another one. More formally, this is
    written as follows:

#+begin_src latex
  a < b \Rightarrow b \nless a
#+end_src

*** Predecessors
    The way in which  Historical Modeling puts facts into a  partial order is to
    identify /predecessors/.  For each fact,  a historical model  makes explicit
    which other facts must have come before. These aren't simply the list of all
    other facts that have occurred earlier in  time: that would put facts into a
    sequence -  a total order.  Instead, predecessors  are facts that  must have
    happened before in order to make the current fact make sense.

    Predecessors are  not simply facts that  occurred earlier in time;  they are
    prerequisites: things that must have been true for this fact to make sense.

*** Successors
    It  is useful  to  talk  about the  opposite  direction  of the  predecessor
    relationship.   A    fact   that    refers   to    another   one    is   its
    /successor/. Successors help us to evolve our understanding of a system over
    time. We cannot change a historical fact, but we can create successors.

    The presence  of a successor does  not change the predecessor.  However, the
    successor changes our interpretation of the predecessor.

    *It is important to recognize that there is no mechanism within a historical
    model to  /prevent/ the creation  of additional successors. If  we carefully
    control /who/ can create successors, and  on what machine, then we can avoid
    this situation in any practical scenario*.

    A fact  does not know  about its successors.  New successors are  added over
    time. To fully understand the state of  a fact, we must query the historical
    model to discover if new successors  have been created. Current state is not
    a projection  of historical  facts into  mutable objects;  it is  simply the
    collection of known successors.

*** Immutable graphs
    Like an event, a  historical fact is immutable. But unlike  an event, a fact
    refers  to   its  predecessors.   Taken  together,  these   properties  have
    interesting consequences.

    The  predecessors  to  which  a  fact refers  to  are  themselves  immutable
    facts. Those facts can in turn  have predecessors. This produces a structure
    known as a  /directed graph/. Each vertex  in this structure is  a fact, and
    each edge is a predecessor  relationship. This relationship has a direction:
    it points from the successor to the predecessor.

    Since  a fact  refers to  its predecessors,  and the  fact is  immutable, it
    follows  that a  predecessor  cannot  be added  to  an  existing fact.  That
    predecessor  relationship is  part  of  the fact,  and  the  fact cannot  be
    modified. And so while it is possible to add successors to a fact, it is not
    possible  to add  predecessors.  This is  in  keeping with  our  use of  the
    predecessor  relationship  to  define  what  comes  before  in  the  partial
    order. All predecessors must be known facts, recorded before the new one.

    From any given fact, we can trace  the graph along the predecessor paths. We
    will  select  a  subgraph  that  includes the  starting  fact,  all  of  its
    predecessors,  and  all  of  their predecessors  recursively.  This  process
    produces the /transitive closure/ of the starting fact.

    To build  the transitive  closure, we  started from  one immutable  fact and
    followed arrows  only in  a direction  that cannot  change. The  subgraph is
    therefore immutable. For any given  fact, the transitive closure will always
    be the same. Adding new successor to any  of the facts in the graph will not
    change it. Those successors would never get added to the transitive closure.

    Conversely, the transitive closure identifies the starting fact. There is no
    other fact for which the transitive  closure would produce this same set. In
    a historical  model, this is the  only way to  identify a fact. They  do not
    have globally unique identifiers (GUIDs) or sequence numbers outside of this
    structure.  The contents  of the  facts in  the transitive  closure are  all
    you've got to tell one fact apart from another.

*** Collaboration
    Machines within a distributed system can communicate by exchanging graphs of
    historical  facts. As  they do,  they must  be sure  to send  the transitive
    closure of each fact.  They have to know that the recipient  is aware of all
    of the predecessors at every step.

    When a machine records  a new piece of information — a  decision that a user
    has made or the outcome of some business  process — it does so by creating a
    new fact. It  cannot create that fact  based on predecessors of  which it is
    not  yet aware.  It must  either create  those predecessors  first, or  have
    learned about them from its peers.

    The  predecessor  relationship  between  facts  captures  the  communication
    structure between  machines. A successor from  one machine can be  seen as a
    response  to its  predecessor generated  on  another. When  you observe  the
    predecessor/successor relationship, you have  evidence that the two machines
    communicated  to  make that  happen.  Conversely,  when  two facts  are  not
    related, then  the two facts might  have been created concurrently.  This is
    the  partial   order  of  historical   facts  at  play   within  distributed
    systems.  The  ambiguity of  the  ordering  between unrelated  facts  leaves
    machines  less  constrained  and,  as  we  will  see,  better  able  to  act
    autonomously.

*** Acyclic Graphs
    The immutability of facts constrains them  to know their predecessors at the
    time of creation. But there are two  more constraints that we have to put on
    the system. *First, we have to be able  to construct the graph one fact at a
    time.  And  second,  we  cannot  allow  a fact  to  refer  to  itself  as  a
    predecessor. We must disallow both simultaneous creation and self-reference,
    lest we introduce cycles*.

    Every graph starts empty. It contains no  facts. The first fact added to the
    graph therefore  can have  no predecessors. There  is no  existing knowledge
    upon which to build.  The first fact is a root. A  graph containing only one
    root has no cycles, because there are no edges.

    Let time  pass, and let more  facts be added  to the graph. Assume  that the
    graph still contains no cycles. As I add  a new fact to the graph, that fact
    may  refer to  any of  the existing  facts as  predecessors. However,  those
    existing facts  may not refer  to this new fact  as a predecessor.  I cannot
    change  their predecessor  relationships,  and  this new  fact  did not  yet
    exist. I therefore cannot introduce a cycle by adding a single fact.

    If  we were  to  allow self-reference,  then we  could  introduce a  trivial
    cycle.  And if  we  were  to allow  simultaneous  insertion,  then we  might
    introduce two facts  that have each other as predecessors.  Since neither of
    these operations is  allowed, the resulting graph of facts  must not contain
    cycles.  In mathematics,  this kind  of structure  is known  as a  *directed
    acyclic graph*, and  has many interesting properties. As we  get deeper into
    the analytical  and implementation details  of historical modeling,  we will
    take full advantage of the acyclic nature of the graph.

*** Timeliness
    In a system based on the exchange  of historical facts, not all parties will
    know about all facts at the same time. This is one of the greatest strengths
    of historical  modeling, but also  one of  its important limitations.  It is
    impossible to reject a fact based on the  time at which you learn of it. The
    reason is  that other parties  will have larned  about it earlier  and would
    therefore have  come to  a different  conclusion about  the fact.  For every
    party in the system to eventually reach the same conclusion, that conclusion
    cannot be based on timeliness.

    This  causes significant  problems in  systems  that do  not recognize  this
    limitation.   Several  legal  documents,  such as  tax  forms,  checks,  and
    invoices,  have explicit  due  dates or  expiration dates.  If  the form  is
    received after  the required date, then  it will not be  honored. The sender
    must  go  to great  lengths  to  prove that  the  document  was written  and
    transmitted on time, or suffer the consequences of a failed transaction.  In
    such  situations,   the  sender  believes   one  thing—that  they   met  the
    deadline—while the recipient believes something else. Only by arbitration of
    a central authority can these situations be resolved.

    To design  a system  that does not  rely upon a  central authority,  we must
    respect that documents will be received late. In a truly historical model, a
    fact is not rejected based on the time at which it was received. At best, we
    can record the fact that a fine was levied or an opportunity was lost due to
    the  failure of  information  to arrive  at  a certain  place  by a  certain
    time. But we cannot prove that  the information did not exist somewhere else
    at that time. And when the fact arrives  later, we must decide how we are to
    react to it.  All parties must honor  the existence of the  facts, no matter
    when they  learned about them,  and draw  the same conclusion.  Perhaps that
    conclusion is  that the sender still  owes a fine. But  timeliness alone did
    not determine that outcome.

    Such are  the rules of  a historical model.  They follow logically  from the
    desire  to capture  the  full  history of  a  system  with several  parties,
    separated by time  and space, exchanging historical facts.  Those facts must
    be immutable.  Two facts having the  same transitive closure are  indeed the
    same fact.  We cannot guarantee—and  therefore cannot rely  upon—there being
    only  one  successor   for  any  given  fact.  And  we   cannot  change  our
    interpretation of history based on the timeliness of our knowledge of it

** Limitations of Historical Modeling
   With  the  power  of  historical   modeling  comes  some  constraints.  These
   constraints make  it inappropriate  to apply  historical modeling  to certain
   types of systems. In these situations, it is best to model all or part of the
   system statically - that is, using a method that captures current state - and
   integrate  where appropriate.  Fortunately, good  integration strategies  are
   available.

   We  will often  find that  we can  pair a  historical model  with a  /static/
   model. A static model, as the name implies, is based upon state. The model is
   mutable, centralized, and can enforce serialized access. Relational databases
   are  good static  models, as  they  have a  long track  record of  supporting
   efficient locking.

*** No central authority
    A  historical model  allows for  decisions to  be made  with autonomy.  Each
    decision is  recorded in the  local history  and eventually shared  with the
    rest of the system. As a result, the system cannot reject facts based on age
    or current state.

    This  makes historical  modeling inappropriate  for parts  of a  system that
    require  a central  authority. For  example, a  conference room  reservation
    system will need  to know with certainty  whether a room was  available at a
    certain time.  When a reservation  is approved,  the approver needs  to know
    that  no other  reservation for  the same  room at  the same  time has  been
    approved. That decision must be made by a central authority.

*** No real-time clock
    A  time-sensitive request  must be  fulfilled within  a specified  period of
    time. If  it is  not, the request  is invalid. Such  requests are  common in
    real-time systems such  as factory automation. A request for  a door to open
    or a robotic  arm to move must be  fulfilled with a narrow span  of time. If
    the message does not arrive in time, then the request must be rejected.

    Facts in  a historical model, however,  are honored no matter  what the time
    frame. The decision is made at the time that the fact is recorded and cannot
    be  rejected thereafter.  It may  take an  indeterminate period  of time  to
    transmit the  fact. The recipient is  simply informed of something  that has
    already happened in history.

*** No uniqueness constraints
    In  a historical  model, any  query for  successors of  a fact  might return
    multiple results. It is not possible to constrain a query to return only one
    result. The consequence of  this is that a domain that  requires at most one
    result cannot effectively be modeled historically.

    For example, a login that requires a unique user name should be supported by
    a static model. A historical model would be unable to enforce the uniqueness
    of a user name.

    To  model a  system that  requires uniqueness  constraints, you  must use  a
    static model. The  model can be consulted to determine  if the desired value
    is already in use.

    If uniqueness is required, such as registering for a user name, a historical
    model could be used for registration  requests, as well as for acceptance or
    rejection responses. The requests can be recorded as facts by clients at the
    edge of the system.  These facts will make their way  to a central authority
    that has  access to  a static  model. The  static model  enforces uniqueness
    constraints. That central authority will decide whether to approve or reject
    the request based on the the static model and then record that decision as a
    successor fact.

*** No aggregation
    After a certain amount of activity, a system might be expected to provide an
    aggregate or  summary of  that period's activity.  For example,  a financial
    ledger could  be closed at the  end of the day,  a month, or a  quarter. The
    system would then produce a summary  that records the total of that period's
    transactions. From that  point forward, no additional  transactions would be
    allowed into that period.

    A historical  model cannot guarantee  that all  facts within a  given period
    have been  seen. The system  responsible for generating the  aggregate might
    not have all of the period's records  at the required time. If it receives a
    fact after computing  and recording the summary, then it  is not permitted -
    by the rules  of historical modeling -  to reject it. The  decision was made
    elsewhere, and the fact of that decision was merely shared.

* Chapter 3 - How to read a historical model
  Throughout  the rest  of this  book,  we will  be exploring  many examples  of
  historical models. To do so, we wil  need a language for describing them. This
  language  will be  part visual  and part  textual. The  visual aspect  of this
  language  will aide  in overall  understanding,  while the  textual part  will
  provide specificity.

  [...] The modeling language that we  will describe - Historical Modeling - has
  two graphical components and one textual component.

** Fact type graphs
   Within the  visual language,  we will  create two kinds  of graphs.  One will
   represent the types of facts, and  the other instances. Fact type graphs will
   be the more common of the two, so let's describe them first.

   In a fact type graph, the type of a fact is represented as a labeled ellipse,
   as in the next figure.

#+begin_src dot :file imgs/a-single-fact-type.png :cmdline -Kdot -Tpng
  digraph G {
    { 
      node [margin=0 fontcolor=blue fontsize=32 width=0.5 shape=circle style=filled]
    }
    Product
  }
#+end_src

#+RESULTS:
[[file:imgs/a-single-fact-type.png]]

   An   arrow  between   two  fact   types  indicates   a  predecessor/successor
   relationship.

   #+begin_src dot :file imgs/arrows-point-up-toward-predecessors.png :cmdline -Kdot -Tpng
     digraph G {
             rankdir=BT
             order [label="Order Line"]
             product [label="Product"]
             order -> product
     }
   #+end_src

   #+RESULTS:
   [[file:imgs/arrows-point-up-toward-predecessors.png]]
   
* Chapter 4 - Location Independence
  This chapter starts the second part of the book, that is called "Application".

  In the not too distant past, most programs ran on a single computer. After the
  proliferation of  JavaScript in the  web browser,  apps on mobile  phones, and
  microservices in the cloud, most programs  that we write today run across many
  computers. Whereas distributed systems used to  be a specialty, today they are
  the default. We need to update other defaults to meet that demand.

  One of the defaults  that we need to update is the assumption  that data has a
  location. Some systems try to treat remote  objects as if they are local. DCOM
  (Distributed Component Object  Model) uses object identifiers to  make a proxy
  look like a  local instance of a remote object.  Remote procedure calls (RPCs)
  try to  hide the  reality of  network communication  behind an  interface that
  looks like a normal function.

  [...] Even  when we replace  RPCs with  messages, and object  identifiers with
  URLs, it is easy  to assume that data has a location.  We make that assumption
  whenever we identify:

  + source of truth
  + system of record

  We rely upon location whenever a single node generates unique identifiers. Our
  default mode of programming what happens at a machine leaks into the behaviors
  that we program into the system as a whole.

  So many  of the behaviors  that we've come to  expect from our  systems depend
  upon  location. We  expect items  to be  sequentially ordered.  We expect  the
  system  to reject  duplicate names.  We expect  that when  the user  updates a
  property of an object, it will have the value that they just assigned. Indeed,
  the   expectation  that   properties  to   even  have   single  values   is  a
  location-dependent assumption.

  A system that depends upon location  will misbehave when that location becomes
  unavailable. If we strive instead for location independence, we will construct
  systems  that are  more  responsive,  resilient, and  reliable.  They can  act
  autonomously  without  communicating  with  remote nodes.  They  can  tolerate
  network failures  without introducing defects.  And the decisions that  a user
  makes in isolation  will be honored when  other nodes and users  learn of that
  decision.

** Synchronization
   [...] A  location-independent system  is not concerned  with synchronization,
   but with  causality. It seeks to  understand which events caused  which other
   events.  Where synchronization  describes  the agreement  of data  structures
   stored in  different locations, causality  describes the history of  the data
   itself, no matter  where it is stored. Causality is  a weaker constraint than
   synchronization, but one that is much easier to achieve.

** Exploring contracts
   We need to do some trade-offs in order to get a reliable system.

*** Identity
    [...] To achieve the best results,  users should be able to identify objects
    just as easlity from any location, without the need to communicate.

**** Auto-incremented IDs
    [...] Auto-incremented  IDs are perfect  for representing identity  within a
    database, although they are a poor  choice for extending identity beyond the
    database. The convenience  of doing so has made them  the default choice for
    identity but has caused many problems downstream.

    The core  of the  issue is  that an  auto-incremented ID  is generated  at a
    certain  location.  It  only  has  a meaning,  initially,  within  a  single
    database.

**** Environment dependence
    IDs generated in different environments does not get translated beyond their
    boundaries, for example, test environment  IDs does not necessarily mean the
    same thing in a  production environment. This can be fixed  by using back up
    and restore  techniques, but  as you  start working with  one system  or the
    other,  the IDs  start  to diverge.  In essence,  you  cannot easily  import
    incrementally  more  data  from  testing without  dropping  the  development
    database.

**** Parent-child insertion
    A parent element have its own primary  key while the child has a foreign key
    that points  to the  parent. In  database systems, we  can't insert  a child
    element while we don't have the parent yet.

    [...] Object relational mappers (ORM)  perform, among other things, the task
    of inserting parent-child relationships. From the outside, it looks as if we
    can build a graph  of objects and then execute a single  command to save the
    changes. But  within the ORM, that  single operation is spread  over several
    batches of INSERT commands, sent to the database in just the right order.

**** Remote creation
    Consider a  mobile application that  stores queries  and actions in  a local
    database   to   work   properly   even  in   a   slow   network   connection
    scenario. Everything  works fine  if this  local database  does not  rely so
    heavily  in auto-incremented  IDs since  this could  generate problems  when
    sychronizing with external database.

*** URLs
    :TODO:

*** Location-independent identity
    When identity is  based on an auto-incremented ID, that  ID only has meaning
    in a  specific location and  can only be  generated there. When  identity is
    based on URLs, the location of the node that responds to subsequent commands
    is given right in the identifier.  When identity is dependent upon location,
    objects show a  certain affinity for their location  of origin. Applications
    start  to have  trouble  using  those objects  when  their locations  become
    unavailable.

    A location-independent identity has three useful properties:

    + It can be generated from any node.
    + It is immutable.
    + It can be compared.

    We have  several ways to solve  this problem and get  a location-independent
    identity.
    
**** Natural keys
     Probably the best example of a  location-independent identity - and the one
     that should be the default in any application design - is the natural key.

**** GUIDs
     When a natural key is not  available, we have mechanisms for generating IDs
     that  do  not  collide  across   machines.  There  are  universally  unique
     identifiers (UUIDs).

     [...]  Originally,  GUIDs were  generated  using  the  MAC address  of  the
     originating machine and  a timestamp. Then, as GUID  generation became more
     frequent,  the timestamp  was  replaced  with a  counter.  Finally, it  was
     recognized that random GUIDs were probably just as good.

**** Timestamps
     Works fine at human scales.

**** Tuples
     Using just  one identity, like  a timestamp, is  often not enough  to avoid
     collisions.  But  bring  different  forms of  identity  together,  and  the
     combination is stronger than  any of its parts. A tuple  is an ordered list
     of values, where each member has its own type and meaning.

**** Hashes
     [...] A hash function takes a tuple  as an input and produces a value.  The
     function  is deterministic:  the same  tuple will  always product  the same
     hash. But ideally, the function should  also be unpredictable: it should be
     hard to find a tuple that produces a given hash.

**** Public keys
     [...]  public  keys are  excellent  ways  to  identify principals  such  as
     individuals or corporations.  Public keys are often used  to digitally sign
     messages,  proving their  authenticity.  Only someone  with  access to  the
     private key could produce the signature.

** Causality
   [...] Causality itself is a hard concept to measure.

   The causes of many events in a  distributed system can be just as complex and
   inscrutable  as  a   chain  of  dominoes.  And  yet  we   still  desire  some
   predictability from the system. And so, we have to find a reasonable stand-in
   for causality that is easier to measure and useful for making predictions.

   While we cannot  always say with certainty that one  event caused another, we
   can say for certain that the cause happened before the effect.

*** Putting steps in order
    We often think about a program as a sequence of steps. [...] When we trey to
    generalize  steps in  a single  program to  multi-threaded or  multi-process
    systems, things get a littler trickier. We cannot say quite so clearly which
    of two steps executing in different processes happened before the other. The
    processes  can  be  running  on   parallel  threads  or  even  on  different
    machines. There is  no single clock that  can help us to put  those steps in
    order.

    We can,  however, observe  that two processes  running independently  do not
    cause  any  behavioral  changes  in  one  another.  They  are  not  causally
    connected. As  long as they don't  communicate, nothing that happens  in one
    can influence the other.

    When  they do  communicate, causality  is clearly  asserted. If  one process
    sends a message, and another process receives it, then we know that the send
    step happened before the receive step. And in a very real sense, the sending
    of a  message caused its receipt.  With this fact  in hand, we can  start to
    causally order steps that have occurred in different processes.

*** The transitive property
    If  one step  happened before  a second,  and the  second happened  before a
    third, then we know that the first happened before the third.

*** Concurrency
    [...] In a very real sense, concurrency is what makes distributed systems so
    difficult to  think about. If there  were no concurrent steps,  we could put
    all of the  steps in order. If  every step can be ordered  relative to every
    other step, then we  would end up with a totally  ordered sequence. It would
    be much easier to think about that kind of system, because it always behaves
    as if the whole network is running on a single machine.

    While a totally ordered system would be  easier to think about, it would not
    have the  properties that we  desire in a  distributed system. It  would not
    scale as we  added more hardware, since totally ordered  steps cannot be run
    in parallel.  It cannot autonomously  serve clients in  different locations,
    because the steps the program takes to serve one client would need to be put
    in order with others  in real time. And it would  not allow for disconnected
    operation, since the steps running on the disconnected computer would be out
    of sequence with the rest of the network.

*** Partial order
    [...] Because sometimes you can tell  and sometimes you can't, the execution
    of steps in a multi-process system is said to be partially ordered.

** The CAP theorem
   + The most famous mathematical idea in all of distributed systems.

   The CAP theorem relates the ideas of consistency, availability, and partition
   tolerance. It is often quoted as saying you can only have two of the three.

*** Defining CAP
    The definition  of *consistency* that  the CAP theorem uses  is specifically
    related  to nodes  in  a distributed  system.  It  says that  if  I ask  two
    different nodes for a value, they will give me the same answer.

    [...] Continuing on, the A in CAP is for *availability*. A node is available
    if it responds in a reasonable amount of time to any request.

    [...]  P is  CAP is  for  *partition tolerance*  and is  related to  network
    partition. A  network partition is  a condition that prevents  messages from
    flowing in a network.

    No distributed system, no matter  what algorithm it uses, can simultaneously
    guarantee consistency,  availability, and  partition tolerance at  any given
    interval.  *If  during that  interval the network  is partitioned,  then the
    system will either be inconsistent or unavailable.*

** Eventual consistency
   If we cannot  expect different nodes within a distributed  system to have the
   same state, then what can we hope to achieve? How can we get any work done if
   we get a different answer from every node that we ask?

   Consistency at any  given instance may be  out of our reach, but  all hope is
   not lost.  We can  achieve consistency  if we  wait long  enough. Eventually,
   nodes will come  into agreement with one another. This  a concept referred to
   as eventual consistency.

   While it  might be desirable to  demand consistency at any  given instant, it
   might not  be practical. If  we loosen our constraints,  we find that  we can
   achieve a much more palatable trade-off.

*** Kinds of consistency
    + Strong consistency:

      That is the garantee that all nodes will report being in the same state at
      any given time.

    + Eventual consistency:

      Mean that nodes will eventually reach the  same state, as long as they can
      continue  to  talk  to  one  another. This  may  require  some  additional
      consensus algorithm, such as conflict resolution. Paxos.

    + Strong eventual consistency:

      Promises that all  nodes reach the same state the  moment they all receive
      the same updates. The nodes do not  need to talk among themselves to reach
      a consensus and resolve conflicts.

*** Strong eventual consistency in a relay-based system
    Taken together,  idempotence and commutativity  are sufficient to  prove SEC
    (Strong Eventual Consistency).

*** Idempotence and commutativity
    Network protocols  have been invented  to specifically try to  address these
    two hard problems:

    + It is hard to guarantee that a message is delivered exactly once - not lost
      not duplicated.
    + It is even harder to guarantee that messages will arrive in the order in which
      they were sent.

** Conflict-free Replicated Data Types (CRDTs)
   We  can  optimize  our  distributed  system  if  we  allow  nodes  to  modify
   messages. Instead of requiring that a node forwards exatcly the same messages
   it received, we can allow the node  to summarize its knowledge and send fewer
   messages.  This is  the strategy  employed by  conflict-free replicated  data
   types (CRDTs).

   A conflict-free replicated  data type is a data structure  that exists not at
   one  location, like  a  typical abstract  data type;  it  exists in  multiple
   locations. Each node in a distributed system has its own replica of the CRDT.

*** State-based CRDTs
    Shapiro, Preguiça,  and colleagues  described two  general solutions  to the
    strong eventual  consistency problem: state-based CRDTs  and operation-based
    CRDTs.

    + Operation-based CRDTs:

      Require a  delivery protocol that ensures  once-and-only-once delivery and
      preserves causal order.  We would prefer  to find a solution that does not
      place  so high  a constraint  on infrastructure  components.  Fortunately,
      state-based   CRDTs   have   no    such   restriction.   State-based   and
      operation-based  CRDTs can  each  emulate one  another  and are  therefore
      equivalent.

*** Partially ordered state
    Each replica  of a state-based  CRDT has  internal state. As  an application
    designer, you  get to  choose the  form of that  internal state.  [...] That
    state has to satisfy a few conditions:

    + Must support causality relationship that defines a partial order.
    + All updates must increase the state in that partial order.
    + It must support a merge operation that takes two states and produces a new
      one that is greater than both of them.
