#+title: Studies
#+author: Me
#+date: 2021-07-31

Org is a  highly flexible structured plain text  file format, composed
of a  few simple, yet versatile,  structures - constructed to  be both
simple enough for the novice and powerful enough for the expert.

+ Tools I want to get better by month:

* 08-2021:
- [x] F#
- [x] Emacs
- [x] Git

* 09-2021:
- [x] Personal story
- [x] SQL practice
- [x] Relational theory

* 10-2021:
- [x] Operational system
- [x] Economics
- [x] Personal story

* 11-2021:
- [ ] Emacs
- [ ] Machine learning

* TODO [F#] F# docs
  :INACTIVE:

** Functions

   Functions are the fundamental unit of program execution in any programming language. In F#,
   all functions are considered values; in fact, they are known as /function values/.

   #+BEGIN_SRC fsharp
     // Non-recursive function definition
     let [inline] function-name parameter-list [: return-type] = function-body
     // Recursive function definition
     let rec function-name parameter-list = recursive-function body
   #+END_SRC

   *Scope*

   At any level of scope other than module scope, it is not an error to reuse a value or function
   name. If you reuse a name, the name declared later shadows the name declared earlier. However,
   at the top level scope in a module, names must be unique.

   *Partial application of arguments*

   If you supply fewer than the specified number of arguments, you create a new function that
   expects the remaining arguments. This method of handling arguments is referred to as /currying/
   and is characteristic of functional programming languages like F#.

   #+BEGIN_SRC fsharp
     let cylinderVolume (radius: float) (length: float) : float =
	 let pi = 3.14159
	 length * pi * radius * radius
     let smallPipeRadius = 2.0
     let bigPipeRadius = 3.0
     let smallPipeVolume = cylinderVolume smallPipeRadius
     let bigPipeVolume = cylinderVolume bigPipeRadius
   #+END_SRC

   Because functions are values, they can be used as arguments to other functions or in other
   contexts where values are used.

   *Lambda expressions*

   A /lambda expression/ in an unnamed function.

   #+BEGIN_SRC fsharp
     let apply (fn: int -> int) value = fn value
     let lambdaFun = fun x -> x + 1
     apply lambdaFun 2
     // result: 3
   #+END_SRC

   *Function composition and pipelining*

   The composition of two functions *fn1* and *fn2* is another function that represents the
   application of *fn1* followed the application of *fn2*.

   #+BEGIN_SRC fsharp
     let fn1 x = x + 1
     let fn2 y = y * 2
     let compfn = fn1 >> fn2
     let result = compfn 100
     // result: 202
   #+END_SRC

   Pipelining enables function calls to be chained together as successive operations. Pipelining
   works as follows:

   #+BEGIN_SRC fsharp
     let result = 100 |> fn1 |> fn2
     // result: 202
   #+END_SRC

   *Recursive functions*

   For some recursive functions, it is necessary to refactor a more "pure" definition to one that
   is [[https://cs.stackexchange.com/questions/6230/what-is-tail-recursion][tail recursive]]. This prevents unnecessary recomputations.

** Classes

   [[https://docs.microsoft.com/en-us/dotnet/fsharp/language-reference/classes][F# docs - Classes]]

   Classes represent the fundamental description of .NET object types; the class is the primary type
   concept that supports object-oriented programming in F#.

* DONE [F#] Learning F# (book)
  CLOSED: [2021-08-15 dom 10:02]

The F# compiler -- which is open source -- compiles your programs into IL, which means that you
can use F# code from any .NET compatible language such as C#; and run it on Mono, .NET Core, or the
.NET framework on windows.

[<EntryPoint>] -> This syntax defines a .NET attribute.

| I'll not continue reading this book since its content is not well explained. |

* TODO [Emacs] Emacs tips
  :INACTIVE:
  
  Ref: [[https://orgmode.org/guide][ORG COMPACT GUIDE]]

** TABLES
  Org comes with a fast and intuitive table editor. Spreadsheet-link calculations are supported in
  connection with the Emacs Calc package. [[https://www.gnu.org/software/emacs/manual/html_node/calc/index.html#Top][GNU Emacs calculator]].

  A table is re-aligned automatically each time you press /TAB/ or /RET/ or /C-c C-c/ inside the
  table. /TAB/ also moves to the next field (/RET/ to the next row) and creates new table rows at
  the end of the table or before horizontal lines. The indentation of the table is set by the first
  line.

  #+BEGIN_SRC org
    |Name|Phone|Age|
    |-
    
    + /TAB/
    
    =
    
    | Name | Phone | Age |
    |------+-------+-----|
    |      |       |     |
  #+END_SRC

  When typing text into a field, Org treats /DEL/, /Backspace/, and all character keys in a special
  way, so that inserting and deleting avoids shifting other fields. Also, when typing immediately after
  point was moved into a new field with /TAB/, the field is automatically made blank.

  *Some commands:*

  /C-c C-c/
    Re-align the table without moving point.

  /TAB/
    Re-align the table, move to the next field. Creates a new row if necessary.

  /S-TAB/
    Re-align, move to previous field.

  /RET/
    Re-align the table and move down to next row. Creates a new row if necessary.

  /S-UP/
  /S-DOWN/
  /S-LEFT/
  /S-RIGHT/
    Move a cell up, down, left, and right by swapping with adjacent cell.

  /M-LEFT, M-RIGHT/
    Move the current column left/right.

  /M-S-LEFT/
    Kill the current column.

  /M-S-RIGHT/
    Insert a new column to the left of point position.

  /M-UP, M-DOWN/
    Move the current row up/down.

  /M-S-UP/
    Kill the current row or horizontal line.

  /M-S-DOWN/
    Insert a new row above the current row. With a prefix argument, the line is created below the
    current one.

  /C-c -/
    Insert a horizontal line below current row. With a prefix argument, the line is created above the
    current line.

  /C-c RET/
    Insert a horizontal line below current row, and move the point into the row below that line.

  /C-c ^/
    Sort the table lines in the region. The position of point indicates the column to be used for
    sorting, and the range of lines is the range between the nearest horizontal separator lines, or
    the entire table.
  
** HYPERLINKS

  Org provides links inside a file, external links to other files, and much more. Also, Org
  recognizes plain URIs, possibly wrapped within angle brackets, and activate them as clickable
  links. The general link format, however, looks like this.
   
  #+BEGIN_SRC org
    [[LINK][DESCRIPTION]]
    
    [[LINK]]
  #+END_SRC

  *Some commands:*

  /C-c C-l/
    Insert a link. This prompts for a link to be inserted into the buffer. You can just type a link,
    or use history keys /UP/ and /DOWN/ to access stored links. You will be prompted for the
    description part of the link.
  
  /C-c C-l/
    Edit the invisible /LINK/ part, with the point on the link.

  /C-c C-o/
    Open link at point.

  /C-c &/
    Jump back to a recorded position. A position is recorded by the commands following internal links,
    and by /C-c %/. Using this command several times in direct succession moves through a ring of
    previously recorded positions.

** /TODO ITEMS/

   Org mode does not require TODO lists to live in separate documents. Instead, TODO items are part
   of a notes file, because they usually come up while taking notes.

   Basically, any headline becomes a TODO item when it starts with the word 'TODO':

   #+BEGIN_SRC org
     **** TODO Write letter to Sam Fortune
   #+END_SRC

   You can use TODO keywords to indicate @emph{sequential} working progress states. [[https://orgmode.org/guide/Multi_002dstate-Workflow.html#Multi_002dstate-Workflow][Doc link]].

   *Checkboxes:*

   Every item in a plain list can be made into a checkbox by starting it with the string '[ ]'.
   Checkboxes are not included into the global TODO list, so they are often great to split a task
   into a number of simple steps.

   *Some commands:*

   /C-c C-t/
     Rotate the TODO state of the current item among
     (unmarked) -> TODO -> DONE -> (unmarked)

   /S-RIGHT/
   /S-LEFT/
     Select the following/preceding TODO state, similar to cycling

   /C-c / t/
     View TODO items in a sparse tree. Folds the entire buffer, but shows all TODO items-with not-DONE
     state-and the headings hierarchy above them.

   /M-x org-agenda t/
     Show the global TODO list. Collects the TODO items (with not-DONE states) from all agenda files
     into a single buffer.

   /S-M-RET/
     Insert a new TODO entry below the current one.

** ORG mode
*** Keep track of the time
   /C-c C-x C-i/ -> clock-in
   /C-c C-x C-o/ -> clock-out

* TODO [F#] SAFE
  :INACTIVE:
  
  The SAFE acronym is made up of four separate components:

  * Saturn -> for back-end services in F#

      The Saturn library builds on top of the solid foundation of both the F#-friendly Giraffe and the
      high performance, rock-solid ASP.NET Core web server to provide a set of optional abstractions
      which make configuring web applications and constructing complex routes extremely easy to
      achieve.

      Saturn can host RESTful API endpoints, drive static websites or server-generated content, all
      inside an easy-to-learn functional programming model.

  * Azure -> as a hosting platform plus associated platform services
  * Fable -> for running F# in the web browser

      Is an  F# to Javascript  compiler, designed to  produce readable
    and standard code. Fable allows you to create applications for the
    browser  written  entirely in  F#,  whilst  also allowing  you  to
    interact with native Javascript as needed.
  
  * Elmish -> for client-side user interfaces

* DONE [DB] Optimistic vs pessimistic lock
  CLOSED:              [2021-08-22             dom              10:53]
  [[https://stackoverflow.com/questions/129329/optimistic-vs-pessimistic-locking/129397#129397][Discussion
  on StackOverflow]]

  - Optimistic locking:

    1. Read a record (with version number) ->
    2. Check that the version hasn't changed ->
    3.1 Write the data (uncorrupted hash);
    3.2 Abort the transaction and the user need to restart it (corrupted hash);

    This strategy is most applicable to high volume systems and three-tier architectures where
    you do not necessarily maintain a connection to the database for your session. In this
    situation the client cannot actually maintain database locks as the connections are taken
    from a pool and you may not be using the same connection from one access to the next.

  - Pessimistic locking:

    1. Lock the record for your exclusive use until you have finished it.

    It has much better integrity with than optimistic locking but requires you to be careful with
    your application design to avoid _deadlocks_. To use pessimistic locking you need either a
    direct connection to the database (as would typically be the case in a two tier client server
    application) or an externally available transaction ID that can be used independently of the
    connection.
    
* [OS] QEMU + NixOS
  With the help of Magueta.

** DONE [[https://www.computerhope.com/jargon/b/bios.htm][BIOS]]
   CLOSED: [2021-08-26 qui 19:08]

   BIOS means short for *Basic Input/Output System*, is a *ROM (Read Only Memory)* chip found on
   motherboards that allows you to access and set up your computer system at the most basic
   level.

   The BIOS includes instructions on how to load basic computer hardware. It also includes a test
   referred to as a POST (Power-On Self-Test) that helps verify the computer meets requirements
   to boot up properly. If the computer does not pass the POST, you head a combination of beeps
   indicating what is malfunctioning in the computer.

   1. POST - Test the computer hardware and make sure no errors exist before loading the OS.
   2. Bootstrap loader - Locate the OS. If a capable OS is located, the BIOS will pass control
      to it.
   3. BIOS drivers - Low-level drivers that give the computer basic operational control over
      your computer's hardware.
   4. BIOS setup or CMOS setup - Configuration program that allows you to configure hardware
      settings including system settings, such as date, time, and computer passwords.

   The BIOS does things like configure the keyboard, mouse, and other hardware, set the system clock, 
   test the memory, and so on. Then it look for a drive and loads the boot loader on the drive, which
   is either an MBR or GPT partition table.
** DONE UEFI
   CLOSED: [2021-08-26 qui 19:08]

   UEFI stands for Unified Extensible Firmware Interface. It is a publicly available specification
   that defines a software interface between an operating system and platform firmware.
   
   UEFI replaces the legacy BIOS firmware interface originally present in all IBM pc's, with most
   UEFI firmware implementations providing support for legacy BIOS services. UEFI can support
   remote diagnostics and repair of computers, even with no operating system installed.
** DONE [[https://www.redhat.com/en/topics/virtualization/what-is-KVM][KVM]]
   CLOSED: [2021-08-26 qui 19:08]

   KVM stands for Kernel-based Virtual Machine. It's an open source virtualization technology
   built into Linux. Specifically, KVM lets you turn Linux into a hypervisor that allows a host
   machine to run multiple, isolated virtual environments called guests or virtual machines (VMs).

   *KVM is part of Linux.*
** DONE [[https://www.qemu.org/][QEMU]]
   CLOSED: [2021-08-26 qui 19:08]

  [[https://qemu-project.gitlab.io/qemu/][Link to the docs.]]

  According to the site, QEMU is a generic and open source machine emulator and virtualizer.

  1. Emulator -

     Hardware or software that enables one computer system (called the host) to behave
     like another computer system (called the guest). An emulator typically enables the host
     system to run software or use peripheral devices designed for the guest system. Emulation
     refers to the ability of a computer program in an electronic device to emulate (or imitate)
     another program or device.
  2. Virtualizer -

     Virtualization means a variety of technologies for managing computer resources
     by providing a software interface, known as an "abstraction layer", between the software
     (operating system and applications) and the hardware. Virtualization turns "physical" RAM
     and storage into "logical" resources.

     2.1. Hardware virtualization -

     This is what most computer people are referring to when they talk about virtualization. It
     partitions the computer's RAM into separate and isolated "virtual machines" (VMs) simulating
     multiple computers within one physical computer. Hardware virtualization enables multiple
     copies of the same or different operating systems to run in the computer and prevents the OS
     and its application in one VM from interfering with the OS and applications in another VM.

     2.2. Network and storage virtualization -

     In a network, virtualization consolidates multiple devices into a logical view so they can
     be managed from a single console. Virtualization also enables multiple storage devices to be
     accessed the same way no matter their type or location.

     2.3. Application virtualization -

     Application virtualization refers to several techniques that make
     running applications protected, flexible and easy to manage.
  
     2.4. OS virtualization -

     Under the control of one operating system, a server is split into
     "containers" that each handle an application.
  
  With this tool it's possible to:
  - Run operating systems for any machine, on any supported architechture.
    It provides a virtual model of an entire machine (CPU, memory and emulated devices) to run
    a guest OS.
  - Run programs for another Linux/BSD target, on any supported architechture.
  - Run KVM and Xen virtual machines with near native performance.

  [[https://www.youtube.com/watch?v=AAfFewePE7c&ab_channel=DenshiVideo][[YouTube - QEMU: A proper guide!]​]]
** DONE Partition information
   CLOSED: [2021-08-26 qui 21:22]

   In this section I'll be sharing other necessary topics to
   understand the complete installation of the NixOS image.
*** Swap memory

    [[https://www.enterprisestorageforum.com/hardware/what-is-memory-swapping/][Ref link.]]

    Memory swapping is a computer techonology that enables an
    operating system to provide more memory to a running application
    or process than is available in physical *random access memory*
    (RAM). When the physical system memory is exhausted, the operating
    system can opt to make use of memory swapping techniques to get
    additional memory.

    Memory swapping works by making use of virtual memory and storage
    space in an approach that provides additional resources when
    required. In short, this additional memory enables the computer to
    run faster and crunch data better.

    With memory swapping, the operating system makes use of storage
    disk space to provide functional equivalent of memory storage
    space.

    The process of memory swapping is managed by an operating system
    or by a virtual machine hypervisor.

    Advantages of memory swapping:

    - More memory: memory swapping is a critical component of memory
management, enabling an operating system to handle requests that would
otherwise overwhelm a system.

    - Continuous operations: swap file memory can be written to disk
in a continuous manner, enabling faster lookup times for operations.

    - System optimization: application processes of lesser importance
and demand can be relegated to swap space, saving the higher
performance physical memory for higher value operations.

    Limitations of memory swapping:

    - Performance: disk storage space, when called up by memory
swapping, does not offer the same performance as physical RAM for
process execution.

    - Disk limitations: swap files are reliant on the stabiity and
availability of storage media, which might not be as stable as system
memory.

    - Capacity: memory swapping is limited by the available swap space
that has been allocated by an operating system or hypervisor.
*** LVM volumes

    In Linux, Logical Volume Manager (LVM) is a device mapper
    framework that provides logical volume management for the Linux
    kernel. Most modern Linux distributions are LVM-aware to the point
    of being able to have their root file systems on a logical volume.
*** Systemd

    [[https://en.wikipedia.org/wiki/Systemd][Reference link.]]

    systemd is a software suite that provides an array of system
    components for Linux operating systems. Its main aim is to unify
    service configuration and behavior across Linux distributions;
    systemd's primary component is a "system and service manager" - an
    init system used to bootstrap user space and manage user
    processes. It also provides replacements for various daemons and
    utilities, including device management, login management, network
    connection management, and event logging. The name systemd adheres
    to the Unix convention of naming daemons by appending the letter d.
*** Software RAID devices

    [[https://en.wikipedia.org/wiki/RAID][Reference link.]]

    RAID stands for "Redundant Array of Inexpensive Disks", is a data
    storage virtualization technology that combines multiple physical
    disk drive components into one or more logical units for the
    purposes of data redundancy, performance improvement, or
    both. This was in contrast to the previous concept of highly
    reliable mainframe disk drives referred to as "single large
    expensive disk" (SLED).
*** UEFI (GPT) x Legacy Boot (MBR)

    [[https://www.freecodecamp.org/news/mbr-vs-gpt-whats-the-difference-between-an-mbr-partition-and-a-gpt-partition-solved/][Reference link.]]

    The main difference between UEFI and legacy boot is that **UEFI** is the 
    latest method of booting a computer that is designed to replace BIOS 
    while the **legacy boot** is the process of booting the computer using
    BIOS firmware.

    Also, UEFI more is recommended because it includes more security features
    (with less complex code) than the legacy BIOS mode.

    GPT and MBR are related to the partition used in the OS.

    Q: So, what's a partition?

    A: Is a virtual division of a hard disk drive (HDD) or a solid state drive
    (SSD). Each partition can vary in size and typically serves a different
    function.

    In Linux there's typically a root partition (`/`), one for swap which helps
    with memory management, and large /home partition. the /home partition is
    similar to the C: partition in Windows in that it's where you install most
    of your programs and store files.

    Program to check the partitions: **GParted**.

    An overview of MBR and GPT partitions

    Before a drive can be divided into individual partitions, it needs to be
    configured to use a specific partition scheme or table.

    A partition table tells the OS how the partitions and data on the drive are
    organized. MBR stands for Master Boot Record, and is a bit of reserved space
    at the beginning of the drive that contains the information about how the
    partitions are organized. The MBR also contains code to launch the OS, and
    it's sometimes called the Boot Loader.

    GPT is an abbreviation of GUID Partition Table, and is a newer standard that's
    slowly replacing MBR. Unlike MBR partition table, GPT stores the data about
    how all the partitions are organized and how to boot the OS throughout the
    drive. That way if one partition is erased or corrupted, it's still possible
    to boot and recover some of the data.

    Some differences:

    * The maximum capacity of MBR partition tables is only about 2 TB. You can use
      a drive that's larger than 2 TB with MBR, but only the first 2 TB of the drive
      will be used. The rest of the storage on the drive will be wasted.

    * In contrast, GPT partition tables offer a maximum capacity of 9.7 ZB, where
      1 ZB = 1 billion TB.

    * MBR partition tables can have a maximum of 4 separate partitions. However,
      one of those partitions can be configured to be an extended partition, which
      is a partition that can be split up into an 23 additional partitions. So the
      absolute maximum number of partitions an MBR partition table can have is 26
      partitions.

    * GPT partition tables allow for up to 128 separate partitions, which is more
      than enough for most real world applications.

    * As MBR is older, it's usually paired with older Legacy BIOS systems, while
      GPT is found on newer UEFI systems. This means that MBR partitions have
      better software and hardware compatibility, though GPT is starting to catch
      up.
** DONE Steps
   CLOSED: [2021-08-26 qui 21:23]
  
  Choose an interface for the system
  - i3wm gaps
  - dwm -> built with C code
  - install the minimum system and install the interface later

  Download the minimal image and configure it to use with QEMU.

  #+BEGIN_SRC bash
    # download the minimal image:
    $ wget https://channels.nixos.org/nixos-21.05/latest-nixos-minimal-x86_64-linux.iso
    # it will download a file named: latest-nixos-minimal-x86_64-linux.iso
    
    # config the image
    # cmd template -> qemu-img create -f qcow2 NOME.img XG
    $ qemu-img create -f qcow2 nixos-test.img 20G
    # command used to create, convert and modify disk images
    # -f:
    #   Stands for format option. qcow2 stands for copy on write 2nd generation.
    
    
    # bootstrap the machine
    # cmd template -> qemu-system-x86_64 -boot d -cdrom image.iso -m 512 -hda mydisk.img
    $ qemu-system-x86_64 -enable-kvm -boot d \
    $ -cdrom latest-nixos-minimal-x86_64-linux.iso \
    $ -m 2G -cpu host -smp 2 -hda nixos-test.img
    # command used to boot an image
    # to get the help use the -h flag
    # -enable-kvm:
    #   Enable KVM full virtualization support. This option is only available if KVM support
    #   is enabled when compiling.
    # -boot
    #   Specify boot order drives as a string of drive letters. Valid drive letters depend on
    #   the target architechture. The x86 PC uses: a, b (floppy 1 and 2), c (first hard disk)
    #   d (first CD-ROM), n-p (Etherboot from network adapter 1-4), hard disk boot is the default.
    # -cdrom
    #   Use file as CD-ROM image (you cannot use -hdc and -cdrom at the same time). You can use
    #   the host CD-ROM by using /dev/cdrom as filename.
    # -m
    #   Set the quantity of RAM.
    # -hda
    #   Use file as hard disk 0, 1, 2 or image.
    
    # start the vm after closing it
    $ qemu-system-x86_64 -enable-kvm -boot d \
    $ -m 2G -cpu host -smp 2 -hda nixos-test.img
  #+END_SRC

  Follow the installation steps provided by the docs. [[https://nixos.org/manual/nixos/stable/index.html#sec-installation][Link here.]]
  
  Some useful keyboard commands:

  - /Ctrl-alt-g/ -> free the mouse from inside the image.
  - /Ctrl-alt-f/ -> toggle switch fullscreen.
* DONE [DB][Course] Basic database concepts
  CLOSED: [2021-09-07 ter 19:48]

- Language used: _Tutorial D_

** Why is faster to do the computations in the database instead of doing with F#?

1. We don't pay the network price.
2. Database runs a series of optimized operations to work with data, generally a
*B-tree* and indexes. When we manipulate data inside F# we are loading everything into
a big chunk of memory. In the best case we will be using O(n) memory where n is the
size of the data.

** Intro

#+BEGIN_SRC bash
  | id | H1 | H2 | H3 | # HEADING
  | ~  | ~  | ~  | ~  | # row content = tuple
  | ~  | ~  | ~  | ~  |
  | ~  | ~  | ~  | ~  |
  | ~  | ~  | ~  | ~  |
  | ~  | ~  | ~  | ~  |

  # table degree = no. of heading (ex.: 4)
  # cardinality = no. of tuples (ex.: 4)
#+END_SRC

Assumptions:

  * Relations never contains duplicate tuples (mathematical set).
  * The tuples of a relation are unordered, top to bottom.
  * The attributes (heading) of a relation are unordered, left to right.
  * Relations (not tables) are always normalized (in 1NF - normal for). Which 
    just means that every tuple in the body conforms to the heading.
  * To perform a join operation the tables must be joinable, i.e.: relations are
    joinable if and only if attributes with the same name are of the same type.
  * Cartesian product is a special case of JOIN. Also, intersect is a special case
    of JOIN as well.

An aggregate operator is not, in general, a relational operator (because the
result usually isn't a relation). It's an operator that derives a single value
from the "aggregate" (i.e., the set or bag) of values of some attribute of some
relation - or, for COUNT, from the entire relation.

  * Integrity constraint

An integrity constraint is, loosely, a boolean expression that must evaluate to
TRUE. This is one of the most important properties of a database. With this we can
trust that the result we are reading from this tool is correct.

System can't enforce truth, can only enforce consistency.

  * Predicates

Heading corresponds to a predicate (truth valued function). Predicates are related
to the understanding of tables in a database.

  * RELATIONS vs. TYPES: TYPES are sets of things we can talk about; RELATIONS are
    (true) statements about those things!

    1. Types and relations are both NECESSARY
    2. They're not the same thing
    3. They're SUFFICIENT (as well as necessary)

A DB (plus its operators) is a logical system!!!

** The relational model:

1. An open ended set of types (including in particular type BOOLEAN)
2. A relational type generator and an intended interpretation for relations of
   types generated thereby
3. Facilities for defining relation variables of such generated relation types
4. A relational assignment operation for assigning relation values to such
   relation variables
5. A relationally complete (but otherwise open ended) set of generic operators
   for deriving relation values from other relation values

** Transactions

A transaction is a piece of program execution: a logical unit of work. Begins by
executing a BEGIN TRANSACTION statement. Ends by executing either a COMMIT or
a ROLLBACK statement.

All database updates (actually database reads too) must be done within the context
of some transaction.

The ACID properties:

1. Atomicity: Transactions are all or nothing. Logical unit of work.
2. Consistency: Transactions transform a consistent state of the DB into another
   consistent state, without necessarily preserving consistency at all intermediate
   points. Logical unit of integrity.
3. Isolation: Any given transaction's update are concealed from all other
   transactions until the given transaction commits. Logical unit of concurrency.
4. Durability: Once a transaction commits, its updates survive in the DB, even
   if there's a subsequent system crash. Logical unit of recovery.

** Database design

Design theory is part of the relational theory in general, but it isn't part of the
relational model as such... It's a separate theory that's built on top of that model.

Recall:

  * Relations are always normalized (i.e., in "1NF"). Which just means every tuple in 
    the body conforms to the heading.
* DONE [F#] F# async model
  CLOSED: [2021-09-15 qua 20:07]
  Produce  a  presentation  about   the  F#  async  model.   Scheduled
  presentation date: 2021-09-16.

References:

[1] - [[https://docs.microsoft.com/en-us/dotnet/fsharp/tutorials/asynchronous-and-concurrent-programming/async#how-to-work-with-net-async-and-taskt][Async programming in F#]] - Very good
[2] - [[https://devblogs.microsoft.com/pfxteam/executioncontext-vs-synchronizationcontext/][ExecutionContext vs SynchronizationContext]] - Too complex
[3] - [[https://docs.microsoft.com/en-us/archive/msdn-magazine/2013/march/async-await-best-practices-in-asynchronous-programming][Async/Await - Best Practices in Asynchronous Programming]] - Too C#/old
[4] - [[https://fsharpforfunandprofit.com/posts/concurrency-async-and-parallel/][Asynchronous programming]] - Very good
[5] - [[https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/async-padl-revised-v2.pdf][The F# Asynchronous Programming Model]] - Very good but with some complex parts
[6] - [[https://github.com/rspeele/TaskBuilder.fs][TaskBuilder.fs docs]] - Very good
[7] - [[http://tomasp.net/blog/csharp-fsharp-async-intro.aspx/][Asynchronous C# and F# (I.): Simultaneous introduction]]
[8] - [[http://tomasp.net/blog/async-csharp-differences.aspx/][Asynchronous C# and F# (II.): How do they differ?]]
[9] - [[http://tomasp.net/blog/async-compilation-internals.aspx/][Asynchronous C# and F# (III.): How does it work?]]

** Theory

*** Definitions:

+ Concurrency: when multiple computations execute in sequential time periods.
+ Parallelism: when multiple computations or several parts of a single computation
  run at exactly the same time.
+ Asynchrony: when one or more computations can execute separately from the main
  program flow. Asynchrony is independent of the utilization of multiple threads.

[1]

*** Etymology of the word "asynchronous":

+ "a", meaning "not".
+ "synchronous", meaning "at the same time".

[1]

*** Asynchronous model within F#:

Since OS threads are expensive  because they allocate system resources
and  large   stacks,  while   lightweight  threading  alone   is  less
interoperable because it slows down  in CPU-intensive native code. And
asynchronous programming  using callbacks  is difficult,  the approach
adopted  by F#  since 2007  is to  add an  asynchronous modality  as a
first-class  feature  to  a  general purpose  language  design,  where
"modality" means  reusing the control  flow syntax of a  host language
with a different computational interpretation.

This modality has control constructs that are syntactically a superset
of  the core  language and  these are  given an  asynchronous semantic
interpretation. For F#, this allows  asynchronous code to be described
fluently  in   familiar  language   syntax,  without   disturbing  the
foundation  of CPU-intensive  programming  that allows  F# to  compile
efficiently  to  Common   IL,  and  hence  to  native   code,  and  to
interoperate well with .NET and C libraries.

[5]

*** Core concepts:

In  F#,  asynchronous  programming   is  centered  around  three  core
concepts:

+ The ~Async<'T>~ type, which represents a composable asynchronous computation.
+ The ~Async~ module functions, which let you schedule asynchronous work, compose
  the asynchronous computations, and transform asynchronous results.
+ The ~async { }~ computation expression, which provides a convenient syntax for
  building and controlling asynchronous computations. All expressions of the form
  ~async {...}~ are of the type ~Async<T>~ for some ~T~.

[1, 5]

Example:

#+BEGIN_SRC fsharp
  open System
  open System.IO

  // string -> Async<unit>
  let printTotalFileBytes path =
    async {
      let! bytes = 
        File.ReadAllBytesAsync(path)
	|> Async.AwaitTask
      let fileName = Path.GetFileName(path)
      printfn $"File {fileName} has %d{bytes.Length} bytes"
    }

 [<EntryPoint>]
 let main argv =
   printTotalFileBytes "path-to-file.txt"
   |> Async.RunSynchronously

   Console.Read() |> ignore
   0
#+END_SRC

[1]

In  F#,  asynchronous   computations  can  be  thought   of  as  *Cold
tasks*. They must be explicitly  started to actually execute. This has
some advantages, as it allows you to combine and sequence asynchronous
work much more easily than in C# or Visual Basic.

*** Practical terms:

In practical terms,  asynchronous computations in F#  are scheduled to
execute *independently of the main program flow*.

This independent execution doesn't imply concurrency or parallelism,
nor does it imply that a computation always happens in the
background. 

In  fact, asynchronous  computations can  even execute  synchronously,
depending on  the nature  of the computation  and the  environment the
computation is executing in.

Although there  are few  garantees about when  or how  an asynchronous
computation executes,  there are some approaches  to orchestrating and
scheduling them.

Example:

#+BEGIN_SRC fsharp
let getWebPage (url: string) = 
  async {
    let req = WebRequest.Create url
    let! resp = req.AsyncGetResponse()
    let stream = resp.GetResponseStream()
    let reader = new StreamReader(stream)
    return! reader.AsyncReadToEnd() }
#+END_SRC

The above example uses several asynchronous operations provided by the
F# library,  namely *AsyncGetResponse*  and *AsyncReadToEnd*.  Both of
these are  I/O primitives  that are  typically used  at the  leaves of
asynchronous operations.

The key  facet of an  asynchronous I/O primitive  is that it  does not
block  an  OS  thread  while  executing,  but  instead  schedules  the
continuation of the asynchronous computation as a callback in response
to an event.

[1, 5]

*** Grammar of asynchronous expressions:

[[/home/gajo/org/imgs/fsharp-async-grammar.png]]

[5]

*** Asynchronous execution:

Because  F#  asynchronous computations  are  a  specification of  work
rather than a  representation of work that is  already executing, they
must be explicitly started with a starting function.

+ Parallel
+ Sequential

[[https://docs.microsoft.com/en-us/dotnet/fsharp/tutorials/asynchronous-and-concurrent-programming/async#important-async-module-functions][Async starting methods]]

[1]

*** Cancellation

A cancellation mechanism  allows computations to be sent  a message to
"stop" execution, e.g. "thread abort" in .NET. Cancellation mechanisms
are  always a  difficult  topic in  imperative programming  languages,
because  compiled,  efficient  native code  often  exhibits  extremely
subtle properties  when pre-emptively  cancelled at  arbitrary machine
instructions.  However, for  asynchronous computations  we can  assume
that primitive asynchronous operations are the norm (e.g. waiting on a
network   request),  and   it  is   reasonable  to   support  reliable
cancellation  at these  operations. Furthermore,  it is  reasonable to
implicitly  support  cooperative  cancellation at  specific  syntactic
points, and additionally through user-defined cancellation checks.

F# async supports  the implicit propagation of  a ~cancellation token~
through   the  execution   of   an   asynchronous  computation.   Each
cancellation  token is  derived  from a  ~cancellation capability~  (a
*CancellationTokenSource*   in  .NET),   used  to   set  the   overall
cancellation condition. A  cancellation token can be given  to lots of
functions.

#+BEGIN_SRC fsharp
  let capability = new CancellationTokenSource()
  let tasks = Async.Parallel [ getWebPage "https://google.com"
			       getWebPage "https://bing.com" ]
  
  // Start the work
  Async.Start (tasks, cancellationToken = capability.Token)
  
  // Ok, the work is in progress, now cancel it...
  capability.Cancel()
#+END_SRC

Cancellation is checked  at each I/O primitive,  subject to underlying
.NET library and O/S support, and before the execution of each return,
let!, use!,  try/with, try/finally, do!  and async { ...  } construct,
and before  each iteration of an  asynchronous while or for  loop. For
getWebPage this means cancellation can occur at several places. But it
cannot  occur  during core-language  code  (e.g.  expressions such  as
library calls, executed for side-effects), and it cannot occur in such
a  way that  the  resource-reclamation  implied by  the  use and  use!
expression  constructs is  skipped.  Cancellation  is not  necessarily
immediately effective: in  a multi-core or distributed  setting it may
take arbitrarily long to propagate the cancellation message.

[5]

*** Exception Handling and Resource Compensation:

Without  a language  support, the  exception handling  in asynchronous
computation is  extremely difficult. With language  support it becomes
simple: the  ~try ... with~  and ~try  ... finally~ constructs  can be
used in async expressions in the natural way:

#+BEGIN_SRC fsharp
async { 
  try
    let! primary = getWebPage "https://primary.server.com"
    return primary.Length
  with e ->
    let! backup = getWebPage "https://backup.server.com"
    return backup.Length
}
#+END_SRC

Here,  a failure  anywhere in  the  download from  the primary  server
results in  the execution of  the exception handler and  download from
the backup server.

+ Definition:

~Deterministic resource disposal~ is a language construct that ensures
that resources  (such as file  handles) are disposed  at the end  of a
lexical scope. In F#  this is the construct *use val  = expr in expr*,
translated to *let val = expr  in try expr finally val.Dispose()*. The
resource *val* is freed on exit from the lexical scope.

Resource  cleanup  in  asynchronous  code is  also  difficult  without
language support. Many OO design  patterns for async programming use a
"state" object to  hold the state elements of  a composed asynchronous
computation,  but this  is non-compositional.  With language  support,
state becomes implied by closure, and resource cleanup becomes simple.

[5]

*** The main differences between _Task_ and _Async_ CE:

This is related to the interoperate  with .NET. C# and the majority of
.NET libraries use the ~Task<TResult>~  and ~Task~ types as their core
abstractions rather  than ~Async<'T>~,  so you  must cross  a boundary
between these two approaches to asynchrony.

You   can  use   *Async.AwaitTask*  to   await  a   .NET  asynchronous
computation,  or  the  *Async.StartAsTask*  to  pass  an  asynchronous
computation to a .NET caller.

You can  use the *Async.AwaitTask*  that accepts  a Task as  input and
this custom  function to start and  await Task types from  an F# async
computation.

#+BEGIN_SRC fsharp
  // Async<unit> -> Task
  let startTaskFromAsyncUnit (comp: Async<unit>) =
    Async.StartAsTask comp :> Task
#+END_SRC

In practice  I have seem most  of the code using  the Task computation
expression  provided  by  the  TaskBuilder.fs to  handle  .NET  ~Task~
s. According  to its docs,  F#'s ~Async~ behaves a  little differently
from ~Task~, which can be confusing  if you're used to the latter. So,
the goal  of the ~task~ computation  expression builder is to  let you
write asynchronous blocks that behave  just like ~async~ methods in C#
do.

[1, 6]

*** Relationship to multi-threading:

  1. There is no affinity between an asynchronous computation and a thread, 
     unless explicitly started  on the current thread.  For example, a
     computation may actually run on its caller's thread, depending on
     the nature of  the work. A computation could  also "jump" between
     threads, borrowing them  for a small amount of time  to do useful
     work in between periods of "waiting" (such as when a network call
     is in transit).

     Although  F# provides  some  abilities to  start an  asynchronous
     computation  on the  current  thread (or  explicitly  not on  the
     current thread),  asynchrony generally  is not associated  with a
     particular threading strategy.

     Each  running computation  in  .NET implicitly  has  access to  a
     synchronization  context, which  for  our purposes  is  a way  of
     taking a function closure and running it "somewhere". We use this
     to execute asynchronous callbacks.

  2. Asynchronous programming in F# is not an abstraction for multi-
     threading.

[1, 5]

*** Use cases:

+ Presenting a server process that can service a significant number of 
  concurrent incoming requests, while minimizing the system resources 
  occupied while request processing awaits inputs from systems or services 
  external to that process.
+ Maintaining a responsive UI or main thread while concurrently progressing 
  background work.

[1]


** Examples

*** How to deal with asynchronous code using callbacks

+ Asynchronous programming using callbacks is difficult.

[5]

+ How to deal with asynchronous code using modern approachs
+ How the context influence the asynchronous (thread)
* DONE [Linux] Terminal commands
  CLOSED: [2021-09-05 dom 20:43]
  
- [x] find
GNU  find   searches  the   directory  tree   rooted  at   each  given
starting-point by evaluating the given  expression from left to right,
according to  the rules of  precedence (see section  OPERATORS), until
the outcome is known (the left  hand side is false for and operations,
true for or), at which point find  moves on to the next file name.  If
no starting-point is specified, `.' is assumed.

- [x] xargs
xargs reads items from the  standard input, delimited by blanks (which
can be  protected with double  or single quotes  or a back‐  slash) or
newlines, and executes the command  (default is /bin/echo) one or more
times with any initial-arguments followed  by items read from standard
input.  Blank lines on the stan‐ dard input are ignored.

- [x] sed
Sed is a stream editor.  A stream editor is used to perform basic text
transformations on an input stream (a  file or input from a pipeline).
While in some  ways similar to an editor which  permits scripted edits
(such as ed), sed  works by making only one pass  over the in‐ put(s),
and is consequently more efficient.  But it is sed's ability to filter
text  in a  pipeline which  particularly distinguishes  it from  other
types of editors.

- [x] cut
Print selected parts of lines from each FILE to standard output.
With no FILE, or when FILE is -, read standard input.

- [x] tr
Translate,  squeeze, and/or  delete  characters  from standard  input,
writing to standard output.

- [x] sort
Write sorted concatenation of all FILE(s) to standard output.
With no FILE, or when FILE is -, read standard input.
* DONE [DB] Postgres lock
  CLOSED: [2021-09-11 sáb 12:17]

[[https://www.citusdata.com/blog/2018/02/15/when-postgresql-blocks/][PostgreSQL rocks, except when it blocks: Understanding locks]]
[[https://www.citusdata.com/blog/2018/02/22/seven-tips-for-dealing-with-postgres-locks/][When Postgres blocks: 7 tips for dealing with locks]]
[[https://skyvia.com/gallery/list-of-all-queries-currently-running-on-postgresql][List of all queries currently running on PostgreSQL]]
[[https://medium.com/little-programming-joys/finding-and-killing-long-running-queries-on-postgres-7c4f0449e86d][Finding and killing long running queries on PostgreSQL]]

** Check PG locks

Sometimes you notice a command is taking awfully long, but the process
is not actually doing anything. In that case it might be waiting for a
lock and you should have a look at *pg_locks*.

To see which query is waiting for a  lock, the PG wiki has a [[https://wiki.postgresql.org/wiki/Lock_Monitoring][number of
useful queries for displaying lock information]].

Get all the information from PG:

#+BEGIN_SRC sql
  SELECT * FROM pg_stat_activity;
#+END_SRC

** Do's and don'ts

1. Never add a column with a default value
   Adding a  column takes a very  aggressive lock on the  table, which
   blocks  read  and write.  If  you  add  a  column with  a  default,
   PostgreSQL will rewrite the whole table  to fill in the default for
   every row, which  can take hours on large tables.  In the meantime,
   all queries will block, so your database will be unavailable.

   #+BEGIN_SRC sql
     -- Don't do this:
     ALTER TABLE items ADD COLUMN last_update timestamptz DEFAULT now();

     -- Do this instead:
     ALTER TABLE items ADD COLUMN last_update timestamptz;
     UPDATE items SET last_update = now();

     -- A better approach would be to update using small batches
     do {
       numRowsUpdated = executeUpdate(
         "UPDATE items SET last_update = ? " +
	 "WHERE ctid IN (SELECT ctid FROM items WHERE last_update IS NULL LIMIT 5000)",
	 now);
     } while (numRowsUpdate > 0);
   #+END_SRC

2. Beware of lock queues, use lock timeouts
   Every lock in PG has a queue. If a transaction B tries to acquire a
   lock that is already held by  transaction A with a conflicting lock
   level,  then  transaction  B  will  wait in  the  lock  queue.  Now
   something interesting  happens: if another transaction  C comes in,
   then it will not  only have to check for conflict  with A, but also
   with transaction B, and any other transaction in the lock queue.

   This means that even if  your DDL command (Data Definition Language
   commands consists  of the SQL commands  that can be used  to define
   database operations) can  run very quickly, it might be  in a queue
   for a  long time waiting  for queries  to finish, and  queries that
   start after it will be blocked behind it.

   #+BEGIN_SRC sql
     -- When you can have long-running SELECT queries on a table, don't do this:
     ALTER TABLE items ADD COLUMN last_update timestamptz;

     -- Instead, do this:
     SET lock_timeout TO '2s'
     ALTER TABLE items ADD COLUMN last_update timestamptz;
   #+END_SRC

   By setting *lock_timeout*, the DDL command  will fail if it ends up
   waiting  for a  lock, and  thus blocking  queries for  more than  2
   seconds. The downside is that your *ALTER TABLE* might not succeed,
   but you can try again later.

   You  may want  to  query  *pg_stat_activity* to  see  if there  are
   long-running queries before starting the DDL command.

3. Create indexes CONCURRENTLY
   Creating an index  on a large dataset can take  hours or even days,
   and the  regular *CREATE INDEX*  command blocks all writes  for the
   duration of the command. While it doesn't block *SELECT* s, this is
   still pretty bad and there's a better way:

   #+BEGIN_SRC sql
     -- Don't do this:
     -- blocks all writes
     CREATE INDEX items_value_idx ON items USING GIN (value jsonb_path_ops);

     -- Instead do this:
     -- only block other DDL
     CREATE INDEX CONCURRENTLY items_value_idx ON items USING GIN (value jsonb_path_ops);
   #+END_SRC

   Creating an index  concurrently does have a  downside. If something
   goes  wrong  it  does  not  roll  back  and  leaves  an  unfinished
   ("invalid") index behind. If that  happens, don't worry, simply run
   *DROP  INDEX CONCURRENTLY  items_value_idx*  and try  to create  it
   again.

4. Take aggressive locks as late as possible
   When you need to run a  command that acquires aggressive locks on a
   table, try to do it as late in the transaction as possible to allow
   queries to continue for as long as possible.

   #+BEGIN_SRC sql
     -- For example, if yu want to completely replace the contents of a table:

     -- Don't do this:
     BEGIN;
     -- reads and writes blocked from here:
     TRUNCATE items;
     -- long-running operation:
     \COPY items from 'newdata.csv' WITH CSV
     COMMIT;

     -- Instead load the data into a new table and then replace the old table:
     BEGIN;
     CREATE TABLE items_new (LIKE items INCLUDING ALL);
     -- long-running operation:
     \COPY items_new FROM 'newdata.csv' WITH CSV
     -- reads and writes blocked from here:
     DROP TABLE items;
     ALTER TABLE items_new RENAME TO items;
     COMMIT;
   #+END_SRC

   There is  one problem, we didn't  block writes from the  start, and
   the old *items* table might have changed by the time we drop it. To
   prevent that, we  can explicitly take a lock the  table that blocks
   writes, but not reads:

   #+BEGIN_SRC sql
     BEGIN;
     LOCK items IN EXCLUSIVE MODE;
     ...
   #+END_SRC

5. Adding a primary key with minimal locking
   Postgres makes  it very easy to  create a primary key  using *ALTER
   TABLE*, but  while the index  for the  primary key is  being built,
   which can take a long time if  the table is large, all queries will
   be blocked.

   #+BEGIN_SRC sql
     -- Don't do this
     -- blocks queries for a long time
     ALTER TABLE items ADD PRIMARY KEY (id);

     -- Do this instead:
     -- takes a long time, but doesn't block queries
     CREATE UNIQUE INDEX CONCURRENTLY items_pk ON items (id);
     -- blocks queries, but only very briefly
     ALTER TABLE items ADD CONSTRAINT items_pk PRIMARY KEY USING INDEX items_pk;
   #+END_SRC

   By breaking down primary key creation into two steps, it has almost
   not impact on the user.

6. Never VACUUM FULL
   The PG user experience can  be a little surprising sometimes. While
   *VACUUM FULL* sounds  like something you want to do  clear the dust
   of your db, a more appropriate command would have been:

   #+BEGIN_SRC sql
     PLEASE FREEZE MY DATABASE FOR HOURS;
   #+END_SRC

   *VACUUM FULL*  rewrites the  entire table to  disk, which  can take
   hours of days,  and blocks all queries while doing  it. While there
   are some  valid use cases for  *VACUUM FULL*, such as  a table that
   used to be big, but is now small and still takes up a lot of space,
   it is probably not your use case.

   While  you should  aim to  tune  your autovacuum  settings and  use
   indexes to make your queries fast, you may occasionally want to run
   *VACUUM*, but NOT *VACUUM FULL*.

7. Avoid deadlocks by ordering commands
   If you've been using PG for a while, chances are you've seen errors
   like:

   #+BEGIN_SRC sql
     ERROR:  deadlock detected
     DETAIL:  Process 13661 waits for ShareLock on transaction 45942; blocked by process 13483.
     Process 13483 waits for ShareLock on transaction 45937; blocked by process 13661.
   #+END_SRC

   This happens when  concurrent transactions take the same  lock in a
   different order. For example:

   #+BEGIN_SRC sql
     -- one transaction issues the following command:
     BEGIN;
     UPDATE items SET counter = counter + 1 WHERE key = 'hello'; -- grabs lock on hello
     UPDATE items SET counter = counter + 1 WHERE key = 'world'; -- blocks waiting for world
     END;

     -- simultaneously, another transaction might be issuing the same commands, but in a different order:
     BEGIN
     UPDATE items SET counter = counter + 1 WHERE key = 'world'; -- grabs lock on world
     UPDATE items SET counter = counter + 1 WHERE key = 'hello';  -- blocks waiting for hello
     END; 
   #+END_SRC

   If these  transaction blocks  run simultaneously, chances  are that
   they get  stuck waiting for each  other and would never  finish. PG
   will recognise this situation after a  second or so and will cancel
   one of  the transactions  to let  the other  one finish.  When this
   happen, you  should take a look  at your application to  see if you
   can  make  transactions  always  follow the  same  order.  If  both
   transactions  first modify  *hello*, then  *world*, then  the first
   transaction will block the second one on the *hello* lock before it
   can grab any other locks.
** Finding and killng long running queries on PG

In order to find them you can use the following query:

#+BEGIN_SRC sql
  SELECT
    pid,
    now() - pg_stat_activity.query_start AS duration,
    query,
    state
  FROM pg_stat_activity
  WHERE (now() - pg_stat_activity.query_start) > interval '5 minutes';
#+END_SRC

If the  state is  idle you don't  need to worry  about it,  but active
queries may be the reason behind low performances on your database.

In order to cancel the long running queries you should execute:

#+BEGIN_SRC sql
  SELECT pg_cancel_backend(__pid__);
#+END_SRC

Where   the   pid   parameter   is   the   value   returned   in   the
*pg_stat_activity*.  It may  take  a  few seconds  to  stop the  query
entirely using the *pg_cancel_backend* command.

If you think that the process is stuck you can kill it by running:

#+BEGIN_SRC sql
  SELECT pg_terminate_backend(__pid__);
#+END_SRC

*Be careful with that!* pg_terminate_backend is  the kill -9 in PG. It
 will terminate the  entire process which can lead to  a full database
 restart in order to recover consistency.
* DONE [DB][Course] Relational theory + SQL
  CLOSED: [2021-09-22 qua 21:29]
** 1. Types
** 2. Relations
** 3. Relational algebra
*** Matching
*** Semijoin
*** Extend
   ~What if supplier status values were tripled?~
*** Image relations
    "Image" in  some relation of some  tuple (usually a tuple  in some
    other relation)
*** Group 
*** SUM (summarization)
*** AVG
*** Relational comparisons
** 4. Relational variables
*** Multiple assignment
   Multiple assignment operator lets  us carry out several assignments
   as a  single operation, without  any integrity checking  being done
   until all assignments have been executed.

   1. Evaluate source expressions
   2. Execute individual assignments "simultaneously"
   3. Do integrity checking

*** Views
    Nice to avoid repeated queries.  .~
** 4.1 The relational model
   1. An open ended collection of types, including in particular type
      BOOLEAN
   2. A relational type generator and an intended interpretation for
      relations of types generated thereby
   3. Facilities for defining relation variables of such generated
      relation types
   4. A relational assignment operation for assigning relation values
      to such relation variables
   5. A relationally complete, but otherwise open ended, collection of
      generic relational operators for deriving relation values from
      other relation values.
** 5. Time and the database
*** Data warehouse
*** Database research on this topic
    A note on the research (there's been some controversy)
    
    Two approaches:
    
    Threat  temporal  data  as  special  and  depart  from  relational
    principles?

    OR...

    Abide firmly by those principles?
** 6. What's the problem with temporal databases
   "Temporal" constraints and queries -  not to mention updates! - can
   be expressed, but they quickly get very complicated indeed.

   We need  some carefully  thought out  and well  designed shorthands
   which typically don't exist in today's commercial DBMSs.
** 7. Intervals
   Crucial  insight: Need  to deal  with intervals  as such  (i.e., as
   values in their own right), instead of pair of FROM-TO values.

   + All those notations represent the same interval:

   [d04:d10] - closed:closed = d04 d05 d06 d07 d08 d09 d10

   [d04:d11) - closed:open = d04 d05 d06 d07 d08 d09 d10

   (d03:d10] - open:closed = d04 d05 d06 d07 d08 d09 d10

   (d03:d11) - open:open = d04 d05 d06 d07 d08 d09 d10

   The table with the interval is being called ~during~.

*** Intervals aren't necessarily temporal
    Tax brackets  are represented by taxable  income ranges (intervals
    whose contained points are money values)

    Machines  operate within  certain temperature  and voltage  ranges
    (intervals whose  contained points are temperatures  and voltages,
    respectively)

    Animals vary in the range of  frequencies of light and sound waves
    to which their eyes and ears are receptive

    Various natural phenomena occur in ranges  in depth of soil or sea
    or height above sea level
** 8. Expand and collapse
   X1_collapsed = {[d1:d5]}

   X1_expanded = {[d1:d1], [d2:d2], [d3:d3], [d4:d4], [d5:d5]}
** 9. Pack and unpack
** 10. Generalizing the relational operators
** 11. Database design I: Structure
*** How do temporal DB design?
    Just add a "temporal" attribute?

    No!

    We propose:

    *Vertical  decomposition*, to  deal  with the  fact that  distinct
    "properties" of the same "entity" vary at different rates.

    *Horizontal decomposition*,  to deal  with the  logical difference
    between current and historical information.
** 12. Database design II: Keys and constraints
** 13. Database design III: General constraints
* TODO [DB] Hackerrank interesting challenges
** Draw the triangle 1
[[https://www.hackerrank.com/challenges/draw-the-triangle-1/problem][Problem link.]]
   
#+BEGIN_SRC sql
set @number = 21;
select repeat('* ', @number := @number - 1) from information_schema.tables;
#+END_SRC
* [DB] Database tips
** Avoid grouping with unnecessary values:
   The keys you pass to group by should have meaning.

   "I will group the commit history  by developer and count the number
of each contributor".

   Compare that to:

   "I  will  group  the  commit history  by  developer  AND  developer
   birthday"

   The developer birthday is a  function of the developer ID. Grouping
by dev  ID and (dev ID,  dev birthday) produces the  same partition on
the  data.  The first  grouping  criteria  is  simpler and  should  be
preferred.

   I think  I've learned this  by being burned  by trying to  group by
more columns than I needed.

   Original:

   #+BEGIN_SRC sql
     Select
	     A.key,
	     A.property,
	     sum(A.value)
	     from
	     A
	     group by A.key, A.property;
   #+END_SRC

   Suggestion to avoid grouping by more than you need:

   #+BEGIN_SRC sql
     Select
	     from
	     A.key,
	     A.property,
	     T.s
	     LEFT JOIN
	     (
	     select
	     A.key,
	     sum(A.value) as s
	     from
	     A
	     group by A.key
	     )as T on A.key = T.key;
   #+END_SRC
* DONE [Course] Inteligência artificial para lideres
  CLOSED: [2021-10-18 seg 19:16]
** Modulo 2
*** Mercado de IA e ML
    Brasil esta começando a aprender a lidar com dados agora. Nestes 4
    anos  a  Datarisk teve  a  oportunidade  de aprender  bastante  (e
    consequentemente errar bastante também).

    IA: Capacidade de simular  raciocínio. Pode ser implementada desde
    em lógicas simples até algoritmos e redes neurais mais complexas.

    ML: Um  dos ramos da IA.  É a capacidade das  máquinas de aprender
    sem que sejame xplicitamente programadas para isso.

    Deep learning: subconjunto

    Sendo  procurado  por empresas  do  mercado  de veículos.  Um  dos
    clientes  é  um aplicativo  de  abastecimento  e a  Datarisk  está
    trabalhando com a prospecção de clientes.

    CAC: Custo de aquisição de cliente.
*** Metodologia de ML
    Plataformas horizontais: PAAS, situações mais gerais.
    
    Plataformas verticais:  Área mais  especializada. A área  que mais
    ajudou a Datarisk a se manter no mercado.

    IA e  ML semicondutores:  desenvolvimento de hardware  com extrema
    eficiência.

    Máquinas autônomas.

    Empresas que usam analytics  demonstram uma performance financeira
    mais sólida.
*** Aplicacoes
*** O que o ML pode ou não fazer
* DONE [DB] DbUp with F#
  CLOSED: [2021-09-25 sáb 20:49]
** What:
   DbUp is  a .NET  library that  helps you to  deploy changes  to SQL
   server  databases.  It  tracks  which SQL  scripts  have  been  run
   already, and  runs the change scripts  that are needed to  get your
   database up to date.
* DONE [Debate] Taskjuggler x Monday
  CLOSED: [2021-09-30 qui 19:32]
  + Present data
  + Focus on the values and vision of the company
    
** Taskjuggler
   *What?*

   Taskjuggler  is  a modern  and  powerful,  __free and  open  source
   software project  management tool__.   Its new approach  to project
   planning and tracking is more flexible and superior to the commonly
   used Gantt chart editing tools.

   It covers  the complete spectrum  of project management  tasks from
   the first  idea to the  completion of  the project. It  assists you
   during  project  scoping,  resource assignment,  cost  and  revenue
   planning, risk and communication management.

   Taskjuggler  provides an  optimizing scheduler  that computes  your
   project time  lines and resource  assignments based on  the project
   outline and the constraints that you have provided.

   ---
   *How?*

   Taskjuggler is written in Ruby and should be easily installable and
   usable on  all popular OS.  It may  sound surprising at  first, but
   this software does  not need a graphical user  interface. A command
   shell, a plain  text editor and a  web browser is all  you need for
   your work.

   --- *Advantages*
   
   1- Integration with Emacs

   2- The  Taskjuggler design  frees the project  manager to  focus on
   information  that  is known  about  the  project  at any  stage  of
   it. Taskjuggler then turns this information into meaningful reports
   and charts.  It supports  the project  manager in  all phases  of a
   project, from the initial idea, to effort estimation, budgeting and
   status tracking.
* [Video][Notes] Joe Armstrong - Keynote: the forgotten ideas in computer science
Youtube video

** Articles
   A plea for lean software - Niklaus Wirth
   The emperor's old clothes - ACM Turing award lecture - Tony Hoare
   
** Tools
   Emacs
   bash
   make
   shell

** Books
   Algorithm data structures programs
   The mythical man-month
   How to win friends and influence people

** Why software is difficult now
   Fast machines
   Huge memory
   Hundreds of PLs
   Distributed
   Huge programs
   No specifications
   Reuse

** Fun programming exercise
   A syntax-oriented compiler writing language

** Great machines from the past
   Baby SSEM
   PDP11
   Vax 11/750
   Cray 1
   IBM PC
   Raspberry PI
   iPhone/iPad
   Nvidia Tesla P100

** Youtube videos to watch
   The computer revolution has not happened yet - Alan Kay
   Computers for Cynics - Ted Nelson
   Free is a lie - Aaron Balkan
   How a handful of tech companies control billions of minds every day - Tristan Harris
   Matt Might - Winning the war on error: solving halting problem, curing cancer - code mesh 2017

** Forgotten ideas
   Linda tuple spaces - David Gelernter and Nicholas Carriero
   Flow based programming - John Paul Morrison
   Xanadu - Ted Nelson
   Unix pipes

** Areas to research
   Robotics
   AI
   Programmer productivity
   Energy efficiency
   Precision medicin
   Security

** Programs to try
   TiddlyWiki
   SonicPI
* TODO [OS][Book] Modern Operating Systems 4ed
  :LOGBOOK:
  CLOCK: [2021-10-06 qua 19:11]--[2021-10-06 qua 19:15] =>  0:04
  CLOCK: [2021-10-06 qua 18:21]--[2021-10-06 qua 19:10] =>  0:49
  CLOCK: [2021-10-05 ter 19:09]--[2021-10-05 ter 20:03] =>  0:54
  CLOCK: [2021-10-04 seg 19:40]--[2021-10-04 seg 20:30] =>  0:50
  CLOCK: [2021-10-03 dom 21:47]--[2021-10-03 dom 22:40] =>  0:53
  CLOCK: [2021-10-02 sáb 11:02]--[2021-10-02 sáb 12:02] =>  1:00
  CLOCK: [2021-10-01 sex 21:42]--[2021-10-01 sex 22:21] =>  0:39
  CLOCK: [2021-10-01 sex 20:58]--[2021-10-01 sex 21:20] =>  0:22
  :END:
** 1. Introduction

   The operating system, the most  fundamental piece of software, runs
   in *kernel mode*  (also called *supervisor mode*). In  this mode it
   has  complete  access to  all  the  hardware  and can  execute  any
   instruction the  machine is capable  of executing. The rest  of the
   software runs in *user mode*, in which only a subset of the machine
   instructions is available.

   ---
   Resource management includes  *multiplexing* (sharing) resources in
   two different ways:  in time and in space. When  a resource is time
   multiplexed, different programs or users take turns using it. First
   one  of  them gets  to  use  the  resource,  then another,  and  so
   on. Determining  how the  resource is time  multiplexed -  who goes
   next and for how long - is the task of the operating system.

   The other  kind of multiplexing  is space multiplexing.  Instead of
   the customers taking turns, each one gets part of the resource. For
   example, main memory  is normally divided up  among several running
   programs,  so each  one  can  be resident  at  the  same time  (for
   example, in order  to take turns using the CPU).  Assuming there is
   enough memory  to hold multiple  programs, it is more  efficient to
   hold several  programs in memory  at once  rather than give  one of
   them all of it, especially if it only needs a small fraction of the
   total. Of course,  this raises issues of  fairness, protection, and
   so on, and it is up to  the operating system to solve them. Another
   resource that is space multiplexed is  the disk. In many systems, a
   single  disk   can  hold  files   from  many  users  at   the  same
   time. Allocating disk space and keeping track of who is using which
   disk blocks is a typical operating system task.

   ---
   To make  it possible to write  programs that could run  on any UNIX
   system, IEEE developed a standard for UNIX, called POSIX, that most
   versions of UNIX  now support. POSIX defines  a minimal system-call
   interface that conformant UNIX systems  must support. In fact, some
   other operating systems now also support the POSIX interface.

   ---
   * Processors

   The brain of the computer is  the CPU. It fetches instructions from
   memory and executes them. The basic  cycle of every CPU is to fetch
   the first instruction from memory,  decode it to determine its type
   and  operands, execute  it,  and then  fetch,  decode, and  execute
   subsequent instructions.  The cycle  is repeated until  the program
   finishes. In this way, programs are carried out.

   Each  CPU  has   a  specific  set  of  instructions   that  it  can
   execute. Thus an *x86* processor  cannot execute *ARM* programs and
   an *ARM* processor cannot execute *x86* programs.

   Because acessing  memory to get  an instruction or data  word takes
   much longer  than executing an  instruction, all CPUs  contain some
   registers inside to hold key variables and temporary results. Thus,
   the instruction set generally contains  instructions to load a word
   from memory into a register, and  store a word from a register into
   memory.

   Special registers that are visible to the programmer:

   *Program  counter*:  contains  the   memory  address  of  the  next
   instruction to be fetched. After that instruction has been fetched,
   the program counter is updated to point to its successor.

   *Stack  pointer*:  points  to  the  top of  the  current  stack  in
    memory. The stack  contains one frame for each  procedure that has
    been entered but  not yet exited. A procedure's  stack frame holds
    those input  parameters, local variables, and  temporary variables
    that are not kept in registers.

   *PSW  (Program Status  Word)*:  Contains the  condition code  bits,
    which are  set by comparison  instructions, the CPU  priority, the
    mode  (user  or kernel),  and  various  other control  bits.  User
    programs may normally read the  entire PSW but typically may write
    only some of its fields. The PSW plays an important role in system
    call and I/O.

   To  improve  performance, CPU  designers  have  long abandoned  the
   simple model  of fetching, decoding, and  executing one instruction
   at a time. Many modern CPUs have facilities for executing more than
   one instruction  at the same  time. For  example, a CPU  might have
   separate  fetch, decode,  and execute  units, so  that while  it is
   executing instruction n, it could  also be decoding instruction n +
   1 and fetching instruction n +  2. Such an organization is called a
   *pipeline*.

   Even  more  advanced than  a  pipeline  design is  a  *superscalar*
   CPU.  In this  design, multiple  execution units  are present,  for
   example,  one  for  integer   arithmetic,  one  for  floating-point
   arithmetic,   and  one   for  Boolean   operations.  Two   or  more
   instructions  are  fetched at  once,  decoded,  and dumped  into  a
   holding buffer until they can be  executed. As soon as an execution
   unit becomes  available, it looks in  the holding buffer to  see if
   there is  an instruction it can  handle, and if so,  it removes the
   instruction from the buffer and executes it. An implication of this
   design  is that  program  instructions are  often  executed out  of
   order. For the most part, it is up to the hardware to make sure the
   result produced is  the same one a  sequential implementation would
   have produced, but an annoying  amount of the complexity is foisted
   onto the operating system.

   User programs always run in user  mode, which permits only a subset
   of the instructions to be executed  and a subset of the features to
   be accessed.  Generally, all instructions involving  I/O and memory
   protection are disallowed in user mode. Setting the PSW mode bit to
   enter kernel mode is also forbidden, of course.

   To obtain services  from the operating system, a  user program must
   make a *system  call*, which traps into the kernel  and invokes the
   operating system. The  TRAP instruction switches from  user mode to
   kernel mode and starts the operating system. When the work has been
   completed,  control  is  returned  to   the  user  program  at  the
   instruction following the system call.

   * Memory

   The memory system is constructed as  a hierarchy of layers. The top
   layers have  higher speed, smaller  capacity, and greater  cost per
   bit than the lower ones, often by factors of a billion or more.

   The top layer  consists of the registers internal to  the CPU. They
   are made of the same material as  the CPU and are thus just as fast
   as the CPU. Consequently, there is  no delay in accessing them. The
   storage capacity available  in them is typically 32 x  32 bits on a
   32-bit CPU and 64  x 64 bits on a 64-bit  CPU. Programs must manage
   the registers  (i.e., decide that  to keep in them)  themselves, in
   software.

   Next  comes the  cache memory,  which is  mostly controlled  by the
   hardware.

   Main memory is usualy called RAM (*Random Access Memory*).

   * Disks

   Many  computers suppor  a scheme  known as  *virtual memory*.  This
   scheme  makes it  possible to  run larger  than physical  memory by
   placing them on the  disk and using main memory as  a kind of cache
   for  the   most  heavily  executed  parts.   This  scheme  requires
   re-mapping memory addresses  on the fly to convert  the address the
   program generated to the physical address  in RAM where the word is
   located. This mapping is done by a  part of the CPU called the *MMU
   (Memory Management Unit)*.

   The presence  of caching  and the  MMU can have  a major  impact on
   performance. In a multiprogramming  system, when switching from one
   program to another, sometimes called  a *context switch*, it may be
   necessary to  flush all modified  blocks from the cache  and change
   the  mapping registers  in the  MMU.  Both of  these are  expensive
   operations, and programmers try hard to avoid them.

   * I/O

   The software  that talks  to a controller,  giving it  commands and
   accepting responses,  is called a *device  driver*. Each controller
   manufacturer has  to supply a  driver for each operating  system it
   supports. Thus a scanner may come with drivers for OS X, Windows 7,
   Windows 8, and Linux, for example.

   To be used, the  driver has to be put into  the operating system so
   it can  run in kernel  mode. Drivers  can actually run  outside the
   kernel, and  operating systems like  Linux and Windows  nowadays do
   offer some support  for doing so. The vast majority  of the drivers
   still run below the kernel boundary. Only very few current systems,
   such as  MINIX 3, run  all drivers in  user space. Drivers  in user
   space must  be allowed to  access the  device in a  controlled way,
   which is not straighforward.

   Input  and output  can  be done  in three  different  ways. In  the
   simplest method,  a user  program issues a  system call,  which the
   kernel then  translates into  a procedure  call to  the appropriate
   driver. The  driver then starts  the I/O and  sits in a  tight loop
   continuously polling the device to see if it is done (usually there
   is some bit that indicates that the device is still busy). When the
   I/O has completed, the driver puts the data (if any) where they are
   needed and  returns. The operating  system then returns  control to
   the  caller. This  method  is  called *busy  waiting*  and has  the
   disadvantage of  tying up the  CPU polling  the device until  it is
   finished.

   The second method is for the driver  to start the device and ask it
   to give an interrupt when it  is finished. At that point the driver
   returns. The operating system then blocks the caller if need be and
   looks for other work to do.  When the controller detects the end of
   the transfer, it generates an *interrupt* to signal completion.

   The third  method for doing  I/O makes  use of special  hardware: a
   *DMA (Direct Memory Access)* chip that can control the flow of bits
   between   memory  and   some   controller   without  constant   CPU
   intervention. The  CPU sets up  the DMA  chip, telling it  how many
   bytes to  transfer, the device  and memory addresses  involved, and
   the direction, and lets it go. When the DMA chip is done, it causes
   an interrupt, which is handled as described before.

   * Buses

   The  main  bus  is  the *PCIe  (Peripheral  Component  Interconnect
   Express)* bus. Capable of transferring tens of gigabits per second,
   PCIe is much faster than its predecessors.

   A *shared  bus architecture*  means that  multiple devices  use the
   same wires to  transfer data. Thus when multiple  devices have data
   to send, you need  an arbiter to determine who can  use the bus. In
   contrast,   PCIe    makes   use   of    dedicated,   point-to-point
   connections. A *parallel bus  architechture* as used in traditional
   PCI means that you send each  word of data over multiple wires. For
   instance, in regular PCI buses, a single 32-bit number is sent over
   32 parallel  wires. In constrast to  this, PCIe uses a  *serial bus
   architechture* and  sends all  bits in a  message through  a single
   connection, known as a lane, much like a network packet.

   - *DMI (Direct Media Interface)* bus.

   The   *SCSI    (Small   Computer   System   Interface)*    bus   is
   high-performance bus  intended for fast disks,  scanners, and other
   devices  needing considerable  bandwidth.  Nowadays,  we find  them
   mostly in servers and workstations.

   * Booting the computer

   Very briefly, the  boot process is as follows. Every  PC contains a
   motherboard, and  there it  has a program  called the  system *BIOS
   (Basic  Input  Output System)*.  The  BIOS  contains low-level  I/O
   software, including procedures  to read the keyboard,  write to the
   screen, and do disk I/O, among other things.

   When the computer  is booted, the BIOS is started.  It first checks
   to see how much RAM is installed and whether the keyboard and other
   basic devices are installed and responding correctly. It starts out
   by  scanning the  PCIe  and PCI  buses to  detect  all the  devices
   attached to  them. If the  devices present are different  from when
   the system was last booted, the new devices are configured.

   The  BIOS then  determines  the boot  device by  trying  a list  of
   devices stored in the CMOS memory. The user can change this list by
   entering    a    BIOS     configuration    program    just    after
   booting. Typically, an attempt is made to boot from a USB drive, if
   one  is present.  If that  fails, the  system boots  from the  hard
   disk. The first sector from the boot device is read into memory and
   executed. This sector contains a program that normally examines the
   partition table  at the end of  the boot sector to  determine which
   partition is active.  Then a secondary boot loader is  read in from
   that partition. This loader reads  in the operating system from the
   active partition and starts it.

   The operating system then queries the BIOS to get the configuration
   information. For each device, it checks to see if it has the device
   driver.  If  not,  it  asks  the  user  to  download  it  from  the
   internet. Once it has all  the device drivers, the operating system
   loads them into the kernel. Then it initializes its tables, creates
   whatever background  processes are  needed, and  starts up  a login
   program or GUI.

   ---
   OPERATING SYSTEM CONCEPTS:

   * Processes

   A process is basically a program in execution. Associated with each
   process is its  *address space*, a list of memory  locations from 0
   to some maximum, which the process  can read and write. The address
   space contains the executable program,  the program's data, and its
   stack. Also  associated with  each process is  a set  of resources,
   commonly  including registers  (including the  program counter  and
   stack pointer), a  list of open files, outstanding  alarms, list of
   related processes, and all the  other information needed to run the
   program. A process is fundamentally  a container that holds all the
   information needed to run a program.

   Periodically,  the operating  system  decides to  stop running  one
   process and  start running another,  perhaps because the  first one
   has used up more  than its share of CPU time in  the past second or
   two.

   When a process is suspended temporarily like this, it must later be
   restarted  in   exactly  the  same   state  it  had  when   it  was
   stopped. This means that all  information about the process must be
   explicitly saved somewhere during the suspension.

   In many operating systems, all  the information about each process,
   other than the  contents of its own address space,  is stored in an
   operating  system table  called the  *process table*,  which is  an
   array of structures, one for each process currently in existence.

   Thus, a (suspended) process consists  of its address space, usually
   called the  *core image*  (in honor of  the magnetic  core memories
   used in days of yore), and  its process table entry, which contains
   the  contents of  its  registers  and many  other  items needed  to
   restart the process later.

   The key process-management  system call are those  dealing with the
   creation  and   termination  of   processes.  Consider   a  typical
   example. A process called the  *command interpreter* or shell reads
   commands  from  a  terminal.  The  user has  just  type  a  command
   requesting that a program be compiled.  The shell must now create a
   new  process that  will run  the  compiler. When  that process  has
   finished the  compilation, it executes  a system call  to terminate
   itself.

   If a process can create one or more other processes (referred to as
   *child processes*)  and these  processes in  turn can  create child
   processes, we quickly arrive at a process *tree* structure.

   Related processes that  are cooperating to get some  job done often
   need  to  communicate  with   one  another  and  synchronize  their
   activities.    This   communication    is   called    *interprocess
   communication*.

   Other process system call are  available to request more memory (or
   release unused memory), wait for  a child process to terminate, and
   overlay its program with a different one.

   Each person  authorized to use  a system  is assigned a  *UID (User
   IDentification)* by the system administrator. Every process started
   has the UID of  the person who started it. A  child process has the
   same UID  as its parent.  Users can be  members of groups,  each of
   which has a *GID (Group IDentification)*.

   ---
   * Files

   Processes can  change their working  directory by issuing  a system
   call specifying the new working directory.

   Before a file can  be read or written, it must  be opened, at which
   time the permissions  are checked. If the access  is permitted, the
   system returns a small integer called a *file descriptor* to use in
   subsequent operations. If  the access is prohibited,  an error code
   is returned.

   A *pipe* is  a sort of pseudofile  that can be used  to connect two
   processes.

   ---
   * Protection

   Files in  UNIX are protected by  assigning each one a  9-bit binary
   protection  code.  The  protection  code consists  of  three  3-bit
   fields, one for the owner, one for the other members of the owner's
   group (users are divided into  groups by the system administrator),
   and one for everyone else. Each field  has a bit for read access, a
   bit for  write access, and a  bit for execute access.  These 3 bits
   are  known as  the *rwx  bits*.  For example,  the protection  code
   rwxr-x--x  means that  the owner  can read,  write, or  execute the
   file, other group  members can read or execute (but  not write) the
   file, and  everyone else can  execute (bit  not read or  write) the
   file. For a directory, x indicates search permission.

   ---

   * System calls

   We  have  seen that  operating  systems  have two  main  functions:
   providing abstractions to user programs and managing the computer's
   resource.

   ---

   * System calls for process management

   *Fork* is the only way to create a new process in POSIX. It creates
    an exact duplicate of the original process, including all the file
    descriptors, registers - everything.  After the fork, the original
    process and the copy (the parent  and the child) go their separate
    ways. All the  variables have identical values at the  time of the
    fork, but since the parent's data  are copied to create the child,
    subsequent changes in one of them do not affect the other one.

   The fork call returns a value, which is zero in the child and equal
   to the child's *PID (Process  IDentifier)* in the parent. Using the
   returned PID,  the two processes  can see  which one is  the parent
   process and which one is the child process.

   Now  consider how  fork is  used by  the shell.  When a  command is
   typed, the shell  forks off a new process. This  child process must
   execute the user command. It does this by using the *execve* system
   call, which causes its entire core  image to be replace by the file
   names in its first parameter.  (Actually, the system call itself is
   *exec*,  but  several library  procedures  call  it with  different
   parameters and  slightly different  names. We  will treat  these as
   system calls here.)

   If *exec* seems  complicated, do not despair;  it is (semantically)
   the most complex of all the POSIX system calls.

   Processes in UNIX have their memory divided up into three segments:
   the *text  segment* (i.e.,  the program  code), the  *data segment*
   (i.e., the variables),  and the *stack segment*. Between  them is a
   gap of unused address space.

   ---
   * System calls for file management

   To  read or  write  a file,  it  must first  be  opened. This  call
   specifies the  file name to be  opened, either as an  absolute path
   name or  relative to the  working directory, as  well as a  code of
   O_RDONLY, O_WRONLY, or O_RDWR, meaning open for reading, writing or
   both. To create a new file, the O_CREAT parameter is used.

   Although most programs read and  write files sequentially, for some
   application programs need  to be able to access any  part of a file
   at random.  Associated with each  file is a pointer  that indicates
   the  current   position  in   the  file.  When   reading  (writing)
   sequentially,  it normally  points  to  the next  byte  to be  read
   (written).  The *lseek*  call  changes the  value  of the  position
   pointer,  so that  subsequent  calls  to read  or  write can  begin
   anywhere in the file.

   ---
   * System calls for directory management

   Every  file  in  UNIX  has  a unique  number,  its  i-number,  that
   identifies it. This i-number is an index into a table of *i-nodes*,
   one per file, telling who owns the file, where its disk blocks are,
   and  so on.  A  directory is  simply  a file  containing  a set  of
   (i-number, ASCII name) pairs.

   The mount  system call allows  two file  systems to be  merged into
   one.

   ---
   * System calls miscellaneous

   The  kill system  call is  the way  users and  user processes  send
   signals. If  a process  is prepared to  catch a  particular signal,
   then when  it arrives, a signal  handler is run. If  the process is
   not prepared to handle a signal, then its arrival kills the process
   (hence the name of the call).

   ---
   * Virtual machines

   In practice, the real distinction between a type 1 hypervisor and a
   type 2 hypervisor is that a type  2 makes uses of a *host operating
   system* and its  file system to create processes,  store files, and
   so  on. A  type 1  hypervisor has  no underlying  support and  must
   perform all these functions itself.

   After a  type 2  hypervisor is started,  it reads  the installation
   CD-ROM  (or CD-ROM  iamge  file) for  the  chosen *guest  operating
   system* and isntalls the guest OS  on a virtual disk, which is just
   a  big file  in the  host operating  system's file  system. Type  1
   hypervisors  cannot do  this  because there  is  no host  operating
   system to store  files on. They must manage their  own storage on a
   raw disk partition.

   ---
   * C programming language

   One feature  that C  has that  Java and Python  do not  is explicit
   pointers.

   In theory,  pointers are typed, so  you are not supposed  to assign
   the address of a floating-point  number to a character pointer, but
   in  practice compilers  accept such  assignments, albeit  sometimes
   with a warning.

   Some things that C does not have include built-in strings, threads,
   packages,    classes,   objects,    type   safety,    and   garbage
   collection.  All  storage  in  C is  either  static  or  explicitly
   allocated and released by the  programmer, usually with the library
   functions  malloc and  free.  It  is the  latter  property -  total
   programmer control over memory -  along with explicit pointers that
   makes C attractive for writing operating systems.
** 2. Processes and threads
   :LOGBOOK:
   CLOCK: [2021-10-12 ter 10:45]--[2021-10-12 ter 12:00] =>  1:15
   CLOCK: [2021-10-11 seg 22:23]--[2021-10-11 seg 22:38] =>  0:15
   CLOCK: [2021-10-10 dom 16:13]--[2021-10-10 dom 16:33] =>  0:20
   CLOCK: [2021-10-10 dom 15:32]--[2021-10-10 dom 16:07] =>  0:35
   CLOCK: [2021-10-09 sáb 17:17]--[2021-10-09 sáb 17:58] =>  0:41
   CLOCK: [2021-10-09 sáb 11:07]--[2021-10-09 sáb 11:53] =>  0:54
   CLOCK: [2021-10-08 sex 20:49]--[2021-10-08 sex 21:32] =>  0:43
   CLOCK: [2021-10-08 sex 20:18]--[2021-10-08 sex 20:44] =>  0:26
   CLOCK: [2021-10-07 qui 18:50]--[2021-10-07 qui 19:27] =>  0:37
   :END:

   The most central concept in any operating system is the process: an
   abstraction of a running program.

   In any  multiprogramming system, the  CPU switches from  process to
   process   quickly,   running  each   for   tens   or  hundreds   of
   milliseconds. While, strictly speaking, at  any one instant the CPU
   is running only one process, in the  course of 1 second it may work
   on several of  them, giving the illusion  of parallelism. Sometimes
   people speak of *pseudoparallelism* in this context, to contrast it
   with  the  true  hardware parallelism  of  *multiprocessor*  system
   (which have two or more CPUs sharing the same physical memory).

   ---
   2.1.1 The Process Model

   A process  is just an  instance of an executing  program, including
   the  current   values  of  the  program   counter,  registers,  and
   variables. Conceptually, each  process has its own  virtual CPU. In
   reality,  of course,  the real  CPU  switches back  and forth  from
   process to process, but to understand the system, it is much easier
   to  think  about a  collection  of  processes running  in  (pseudo)
   parallel than  to try to  keep track of  how the CPU  switches from
   program to program.  This rapid switching back and  forth is called
   *multiprogramming*.

   * In this chapter we will assume there is only one CPU.

   The  difference between  a process  and  a program  is subtle,  but
   absolutely  crucial.  An analogy  may  help  you here.  Consider  a
   culinary-minded computer  scientist who  is baking a  birthday cake
   for his young daughter. He has a birthday cake recipe and a kitchen
   well stocked  with all  the input: flour,  eggs, sugar,  extract of
   vanilla, and  so on. In  this analogy,  the recipe is  the program,
   that  is, an  algorithm expressed  in some  suitable notation,  the
   computer scientist is the processor  (CPU) and the cake ingredients
   are the input  data. The process is the activity  consisting of our
   baker reading the recipe, fetching  the ingredients, and baking the
   cake.

   Now  imagine that  the computer  scientist's son  comes running  in
   screaming his head off, saying that he has been stung by a bee. The
   computer scientist records where he was in the recipe (the state of
   the  current process  is saved),  gets out  a first  aid book,  and
   begins following  the directions in  it. Here we see  the processor
   being  switched  from one  process  (baking)  to a  higher-priority
   process  (administering  medical  care), each  having  a  different
   program (recipe versus first aid book). When the bee sting has been
   taken  care of,  the  computer  scientist goes  back  to his  cake,
   continuing at the point where he left off.

   ---
   2.1.2 Process Creation

   Processes that stay in the  background to handle some activity such
   as  email,  Web  pages,  news,  printing,  and  so  on  are  called
   *daemons*.

   In UNIX  there is  only one  system call to  create a  new process:
   *fork*.  After the  fork, the  two  processes, the  parent and  the
   child, have  the same memory  image, the same  environment strings,
   and the same  open files. Usually, the child  process then executes
   *execve* or  a similar system call  to change its memory  image and
   run a new program.

   ---
   2.1.4 Process Hierarchies

   In some systems, when a process creates another process, the parent
   process  and child  process continue  to be  associated in  certain
   ways. The child process can itself create more processes, forming a
   process hierarchy.  Note that  unlike plants  and animals  that use
   sexual reproduction, a process has  only one parent (but zero, one,
   two, or  more children).  So a  process is more  like a  hydra than
   like, say, a cow.

   In UNIX, a process and all  of its children and further descendants
   together form a process group. When  a user sends a signal from the
   keyboard, the  signal is  delivered to all  members of  the process
   group currently  associated with  the keyboard (usually  all active
   processes that  were created in the  current window). Individually,
   each process can  catch the signal, ignore the signal,  or take the
   default action, which is to be killed by the signal.

   As another example of where the process hierarchy plays a key role,
   let us look at how UNIX initializes itself when it is started, just
   after the computer is booted.  A special process, called *init*, is
   present in the boot image. When  it starts running, it reads a file
   telling  how many  terminals there  are. Then  it forks  off a  new
   process  per terminal.  These  processes wait  for  someone to  log
   in. If a login is successful, the login process executes a shell to
   accept commands. These commands may start up more processes, and so
   forth. Thus,  all the  processes in  the whole  system belong  to a
   single tree, with *init* at the root.

   ---
   2.1.5 Process States

   When  a process  blocks, it  does  so because  logically it  cannot
   continue, typically because it is waiting for input that is not yet
   available. It is  also possible for a process  that is conceptually
   ready and  able to run to  be stopped because the  operating system
   has decided to allocate the CPU to another process for a while.

   Three states a process may be in:

   * Running (actually using the CPU at that instant)
   * Ready (runnable; temporarily stopped to let another process run)
   * Blocked (unable to run until some external event happens)

   ---
   2.1.6 Implementation of Processes

   To implement  the process model,  the operating system  maintains a
   table (an  array of structures),  called the *process  table*, with
   one entry  per process. (Some  authors call these  entries *process
   control blocks*).  This entry contains important  information about
   the process  state, including  its program counter,  stack pointer,
   memory allocation, the status of its open files, its accounting and
   scheduling information, and everything  else about the process that
   must be saved when the process is switched from running to ready or
   blocked state so that it can be  restarted later as if it had never
   been stopped.

   A  process  may  be  interrupted  thousands  of  times  during  its
   execution,  but the  key  idea  is that  after  each interrupt  the
   interrupted process returns  to precisely the same state  it was in
   before the interrupt occurred.

   ---
   2.1.7 Modeling Multiprogramming

   When multiprogramming is used, the CPU utilization can be improved.

   Crudely put, if  the average process computes only 20%  of the time
   it is sitting in memory, then with five processes in memory at once
   the CPU should be busy all  the time. This model is unrealistically
   optimistic,  however,  since  it  tacitly  assumes  that  all  five
   processes will never be waiting for I/O at the same time.

   ---
   *THREADS*

   In traditional operating systems, each process has an address space
   and  a single  thread  of  control. In  fact,  that  is almost  the
   definition of  a process. Nevertheless,  in many situations,  it is
   desirable to have  multiple threads of control in  the same address
   space  running  in quasi-parallel,  as  though  they were  (almost)
   separate processes (except for the shared address space).

   ---
   2.2.1 Thread Usage

   Why would anyone  want to have a kind of  process within a process?
   It  turns   out  there  are   several  reasons  for   having  these
   miniprocesses, called *threads*.

   1st argument:

   The main  reason for having  threads is that in  many applications,
   multiple activities are  going on at once. Some of  these may block
   from time to time. By decomposing such an application into multiple
   sequential  threads that  run  in  quasi-parallel, the  programming
   model becomes simpler.

   Instead of thinking about interrupts, timers, and context switches,
   we can think about parallel processes. Only now with threads we add
   a new  element: the ability for  the parallel entities to  share an
   address space and all of its data among themselves. This ability is
   essential for  certain applications,  which is why  having multiple
   processes (with their separate address spaces) will not work.

   2nd argument:

   Since  they are  lighter  weight than  processes,  they are  easier
   (i.e.  faster)  to  create  and destroy  than  processes.  In  many
   systems, creating a thread goes 10-100 times faster than creating a
   process. When the number of  threads needed changes dynamically and
   rapidly, this property is useful to have.

   3rd argument:

   Threads yield no  performance gain when all of them  are CPU bound,
   but when there  is substantial computing and  also substantial I/O,
   having threads allows these activities to overlap, thus speeding up
   the application.

   Finally, threads  are useful on  systems with multiple  CPUs, where
   real parallelism is possible.

   Threads make it possible to retain the idea of sequential processes
   that make  blocking calls  (e.g., for disk  I/O) and  still achieve
   parallelism.  Blocking  system calls  make programming  easier, and
   parallelism  improves   performance.  The   single-threaded  server
   retains  the  simplicity of  blocking  system  calls but  gives  up
   performance. The  third approach  achieves high  performance though
   parallelism but uses  nonblocking calls and interrupts  and thus is
   hard to program.

   [...] A  third example (I  didn't copy the previous)  where threads
   are useful is in applications  that must process very large amounts
   of data. The normal approach is to read in a block of data, process
   it, and then write  it out again. The problem here  is that if only
   blocking system calls are available,  the process blocks while data
   are coming in and  data are going out. Having the  CPU go idle when
   there is lots of computing to  do is clearly wasteful and should be
   avoided if possible.

   Threads offer a  solution. The process could be  structured with an
   input thread, a processing thread,  and an output thread. The input
   thread reads data into an input buffer. The processing thread takes
   data out of the input buffer,  processes them, and puts the results
   in an output buffer. The output buffer writes these results back to
   disk. In this  way, input, output, and processing can  all be going
   on at the same  time. Of course, this model works  only if a system
   call blocks only the calling thread, not the entire process.

   ---
   2.2.2 The Classical Thread Model

   One  way of  looking at  a process  is that  it is  a way  to group
   related  resources  together.  A   process  has  an  address  space
   containing program text and data, as well as other resources. These
   resources may include open  files, child processes, pending alarms,
   signal handlers, accounting information,  and more. By putting them
   together in the form of a process, they can be managed more easily.

   The other concept  a process has is a thread  of execution, usually
   shortened to just  *thread*. The thread has a  program counter that
   keeps track of which instruction to execute next. It has registers,
   which hold  its current  working variables. It  has a  stack, which
   contains the execution  history, with one frame  for each procedure
   called but not yet returned from. Although a thread must execute in
   some process, the thread and its process are different concepts and
   can be  treated separately. Processes  are used to  group resources
   together; threads are  the entities scheduled for  execution on the
   CPU.

   What  threads  add  to  the  process model  is  to  allow  multiple
   executions  to take  place in  the same  process environment,  to a
   large degree  independent of  one another. Having  multiple threads
   running in parallel in one  process is analogous to having multiple
   processes running in parallel in one computer.

   Because threads have some of  the properties of processes, they are
   sometimes called *lightweight processes*.

   The term *multithreading* is also used to describe the situation of
   allowing multiple threads in the same process.

   When a  multithreaded process  is run on  a single-CPU  system, the
   threads  take turns  running.  By switching  back  and forth  among
   multiple  processes,  the system  gives  the  illusion of  separate
   sequential processes running in  parallel. Multithreading works the
   same  way.  The CPU  switches  rapidly  back  and forth  among  the
   threads, providing  the illusion  that the  threads are  running in
   parallel, albeit  on a  slower CPU  than the  real one.  With three
   compute-bound threads in a process,  the threads would appear to be
   running in parallel, each one on  a CPU with one-third the speed of
   the real CPU.

   Different threads in a process  are not as independent as different
   processes. All threads  have exactly the same  address space, which
   means that they also share the same global variables.

   Since  every thread  can  access every  memory  address within  the
   process address space, one thread can read, write, or even wipe out
   another  thread's stack.  There  is no  protection between  threads
   because (1) it is impossible, and (2) it should not be necessary.

   Unlike different processes,  which may be from  different users and
   which may be hostile to one another, a process is always owned by a
   single user,  who has presumably  created multiple threads  so that
   they can cooperate, not fight.

   In addition to sharing the address space, all the threads can share
   the same set  of open files, child processes,  alarms, and signals,
   and so on.

   Like a traditional  process, a thread can be in  any one of several
   states: running, blocked, ready, or terminated.

   It is important to realize that each thread has its own stack. Each
   thread's stack contains one frame for each procedure called but not
   yet  returned  from.  This  frame contains  the  procedure's  local
   variables and the return address to use when the procedure call has
   finished.

   When  multithreading is  present,  processes usually  start with  a
   single thread  present. This thread  has the ability to  create new
   threads by calling a library procedure such as *thread_create*.

   When  a thread  has finished  its work,  it can  exit by  calling a
   library procedure, say, *thread_exit*.  It  then vanishes and is no
   longer schedulable.

   ---
   2.2.4 Implementing Threads in User Space

   There are two main places to  implement threads: user space and the
   kernel.

   The first  method is to put  the threads packages entirely  in user
   space. The kernel knows nothing about them. As far as the kernel is
   concerned, it is managing  ordinary, single-threaded processes. The
   first, and  most obvious,  advantage is  that a  user-level threads
   package can  be implemented  on an operating  system that  does not
   support threads.

   When threads are managed in user  space, each process needs its own
   private  *thread  table* to  keep  track  of  the threads  in  that
   process. This  table is  analogous to  the kernel's  process table,
   except that it keeps track  only of the per-thread properties, such
   as each thread's program  counter, stack pointer, registers, state,
   and  so  forth.  The  thread  table  is  managed  by  the  run-time
   system. When a thread is moved to ready state or blocked state, the
   information needed  to restart  it is stored  in the  thread table,
   exactly  the  same  way  as the  kernel  stores  information  about
   processes in the process table.

   If the  program calls  or jumps  to an instruction  that is  not in
   memory, a  page fault occurs and  the operating system will  go and
   get the missing instruction (and  its neighbors) from disk. This is
   called a *page fault*.

   ---
   2.2.5 Implementing Threads in the Kernel

   When the  kernel knows  about and manage  the threads,  no run-time
   system is  needed in each thread,  and there is no  thread table in
   each process.  Instead, the  kernel has a  thread table  that keeps
   track of  all the  threads in  the system. When  a thread  wants to
   create  a new  thread or  destroy an  existing thread,  it makes  a
   kernel  call,  which  then  does the  creation  or  destruction  by
   updating the kernel thread table.

   Their  main disadvantage  is  that the  cost of  a  system call  is
   substantial, so if thread  operations (creation, termination, etc.)
   a common, much more overhead will be incurred.

   ---
   2.2.6 Hybrid Implementation

   Various  ways  have  been  investigated   to  try  to  combine  the
   advantages of user-level threads with kernel-level threads. One way
   is use  kernel-level threads and then  multiplex user-level threads
   onto some or all of them.

   With this  approach, the kernel  is aware of only  the kernel-level
   threads  and  schedules  those.  Some of  those  threads  may  have
   multiple  user-level  threads multiplexed  on  top  of them.  These
   user-level  threads are  created, destroyed,  and rescheduled  just
   like  user-level threads  in a  process that  runs on  an operating
   system  without  multithreading  capability. In  this  model,  each
   kernel-level thread  has some set  of user-level threads  that take
   turns using it.

   ---
   2.2.7 Scheduler Activations

   While kernel threads are better than user-level threads in some key
   ways, they are also indisputably slower.

   The  goals  of the  scheduler  activation  work  are to  mimic  the
   functionality of  kernel threads,  but with the  better performance
   and greater  flexibility usually  associated with  threads packages
   implemented in user space.

   When scheduler activations  are used, the kernel  assigns a certain
   number  of  virtual   processor  to  each  process   and  lets  the
   (user-space) run-time system allocate threads to processor.

   ---
   2.2.8 Pop-Up Threads

   ---
   2.2.9 Making Single-Threaded Code Multithreaded

   Many   existing   programs   were   written   for   single-threaded
   processes. Converting these to multithreading is much trickier than
   it may at first appear.

   * Few of the pitfalls:

   The code of a thread normally consists of multiple procedures, just
   like a process.  These may have local  variables, global variables,
   and parameters.  Local variables  and parameters  do not  cause any
   trouble, but variables  that are global to a thread  but not global
   to the entire  program are a problem. These are  variables that are
   global in the sense that many procedures within the thread use them
   (as they might  use any global variable), but  other threads should
   logically leave them alone.

   As an example, consider the errno variable maintained by UNIX. When
   a process (or  a thread) makes a system call  that fails, the error
   code is put into errno. Imagine that a thread 1 executes the system
   call access  to find out if  it has permission to  access a certain
   file.  The  operating  system  returns the  answer  in  the  global
   variable errno. After control has  returned to thread 1, but before
   it has a chance to read  errno, the scheduler decides that thread 1
   has had  enough CPU time  for the moment  and decides to  switch to
   thread 2. Thread  2 executes an open call that  fails, which causes
   errno  to be  overwritten  and thread  1's acess  code  to be  lost
   forever. When  thread 1  starts up  later, it  will read  the wrong
   value and behave incorrectly.

   Various solutions to this problem  are possible. One is to prohibit
   global variables altogether.  However worthy this ideal  may be, it
   conflicts with  much existing software.  Another is to  assign each
   thread its own  private global variables. In this  way, each thread
   has its  own private copy of  errno and other global  variables, so
   conflicts are avoided.

   The  next  problem in  turning  a  single-threaded program  into  a
   multithreaded  one   is  that  many  library   procedures  are  not
   reentrant. That  is, they were not  designed to have a  second call
   made  to any  given procedure  while a  previous call  has not  yet
   finished. For example, sending a  message over the network may well
   be programmed to assemble the message  in a fixed buffer within the
   library, then to trap to the kernel to send it. What happens if one
   thread  has assembled  its  message  in the  buffer,  then a  clock
   interrupt  forces a  switch  to a  second  thread that  immediately
   overwrites the buffer with its own message?

   Fixing all  these problems  effectively means rewriting  the entire
   library. Doing so is a  nontrivial activity with a real possibility
   of introducing subtle errors.

   A different  solution is  to provide each  procedure with  a jacket
   that sets  a bit  to mark the  library as in  use. Any  attempt for
   another thread to use a library procedure while a previous call has
   not yet completed is blocked. Although this approach can be made to
   work, it greatly eliminates potential parallelism.

   ---
   2.3 INTERPROCESS COMMUNICATION

   Process frequently  need to  communicate with other  processes. For
   example, in a shell pipeline, the  output of the first process must
   be passed  to the  second process,  and so on  down the  line. Thus
   there is a need for  communication between processes, preferably in
   a  well-structured  way  not  using interrupts.  In  the  following
   sections  we will  look  at  some of  the  issues  related to  this
   *InterProcess Communication*, or *IPC*.

   Very briefly, there are three issues here:

   1. How one process can pass information to another?

   2. How can you make sure two or more processes do not get in each
      other's way, for example, two processes in an airline reservation
      system each trying to grab the last seat on a plane for a
      different customer.

   3. The third concerns proper sequencing when dependencies are present:
      if process A  produces data and process B prints  them, B has to
      wait until A has produced some data before starting to print.

   It is  also important  to mention  that two  of these  issues apply
   equally well to  threads. The first one - passing  information - is
   easy for threads  since they share a common  address space (threads
   in different address spaces that need to communicate fall under the
   heading  of  communicating processes).  However,  the  other two  -
   keeping  out of  each other's  hair and  proper sequencing  - apply
   equally  well to  threads. The  same  problems exist  and the  same
   solutions apply.

   ---
   2.3.1 Race Conditions

   In some operating systems, processes  that are working together may
   share some common storage that each one can read and write.

   Situations where two or more  processes are reading or writing some
   shared  data and  the final  result depends  on who  runs precisely
   when, are  called *race conditions*. Debugging  programs containing
   race conditions is no fun at all. The results of most test runs are
   fine,  but once  in a  blue  moon something  weird and  unexplained
   happens.   Unforunately,  with   increasing   parallelism  due   to
   increasing  numbers  of cores,  race  condition  are becoming  more
   common.

   ---
   2.3.2 Critical Regions

   How do we avoid race conditions? The key to preventing trouble here
   and in many other situations involving shared memory, shared files,
   and shared  everything else is  to find  some way to  prohibit more
   than one  process from reading and  writing the shared data  at the
   same time. Put in other words,  what we need is *mutual exclusion*,
   that is,  some way of  making sure that if  one process is  using a
   shared variable or file, the  other processes will be excluded from
   doing the same thing.

   The problem of  avoiding race conditions can also  be formulated in
   an abstract way. Part of the time, a process is busy doing internal
   computations  and   other  things   that  do   not  lead   to  race
   conditions.  However,  sometimes a  process  has  to access  shared
   memory  or files,  or do  other critical  things that  can lead  to
   races. That part of the program where the shared memory is accessed
   is called the *critical region*  or *critical section*. If we could
   arrange  matters such  that no  two  processes were  ever in  their
   critical regions at the same time, we could avoid races.

   Although  this  requirement  avoids  race  conditions,  it  is  not
   sufficient for  having parallel  processes cooperate  correctly and
   efficiently using shared  data. We need four conditions  to hold to
   have a good solution.

   1. No two processes may be simultaneously inside their critical regions

   2. No assumptions may be made about speeds or the number of CPUs

   3. No process running outside its critical region may block any process

   4. No process should have to wait forever to enter its critical region

   ---
   2.3.3 Mutual Exclusion with Busy Waiting

   *Disabling Interrupts*

   On a single-processor system, the simplest solution is to have each
   process  disable all  interrupts just  after entering  its critical
   region and re-enable them just before leaving it.

   In  a   multicore  (i.e.,  multiprocessor  system)   disabling  the
   interrupts of one CPU does  not prevent other CPUs from interfering
   with  operations the  first CPU  is performing.  Consequently, more
   sophisticated schemes are needed.

   *Lock Variables*

   As a second attempts, let us look for a software solution. Consider
   having  a  single, shared  (lock)  variable,  initially 0.  When  a
   process  wants to  enter its  critical region,  it first  tests the
   lock. If  the lock is 0,  the process sets  it to 1 and  enters the
   critical region. If  the lock is already 1, the  process just waits
   until it  becomes 0.  Thus, a  0 means  that no  process is  in its
   critical region, and a 1 means that some process is in its critical
   region.

   This is sucetive to some  race conditions. Suppose that one process
   reads the lock and sees that it is 0. Before it can set the lock to
   1, another process is scheduled, runs  and sets the lock to 1. When
   the first process runs  again, it will also set the  lock to 1, and
   two processes will be in their critical regions at the same time.

   *Strict Alternation*

   *Peterson's Solution*

   *The TSL Instruction*

   ---
   2.3.5 Semaphores

   In  the  beginning  a  new  variable  type  was  introduced  called
   *semaphore*. A semaphore could have the value 0, indicating that no
   process wakeups were  saved, or more positive value if  one or more
   wakeups were pending.

   Dijkstra proposed having two  operations on semaphores, now usually
   called  down   and  up   (generalizations  of  sleep   and  wakeup,
   respectively). The down  oepration on a semaphore checks  to see if
   the value is greater than 0.  If so, it decrements the value (i.e.,
   uses up one  stored wakeup) and just continues. If  the value is 0,
   the process  is put to  sleep without  completing the down  for the
   moment.  Checking the  value, changing  it, and  possibly going  to
   sleep, are all done as a single, indivisible *atomic action*. It is
   garanteed that  once a  semaphore operation  has started,  no other
   process can access the semaphore  until the operation has completed
   or  blocked.  This atomicity  is  absolutely  essential to  solving
   synchronization problems and avoiding race conditions.

   Atomic actions, in  which a group of related  operations are either
   all performed  without interruption  or not  performed at  all, are
   extremely  important in  many other  areas of  computer science  as
   well.

   ---
   2.3.6 Mutexes

   When the semaphore's  ability to count is not  needed, a simplified
   version   of  the   semaphore,   called  a   mutex,  is   sometimes
   used. Mutexes are  good only for managing mutual  exclusion to some
   shared resource  or piece of code.  They are easy and  efficient to
   implement, which  makes them  especially useful in  thread packages
   that are implemented entirely in user space.

   A *mutex* is  a shared variable that  can be in one  of two states:
   unlocked  or  locked.  Consequently,  only 1  bit  is  required  to
   represent it,  but in  practice an  integer often  is used,  with 0
   meaning unlocked and all other values meaning locked.

   *Futex*

   A futex is  a feature of Linux that implements  basic locking (much
   like a mutex) but avoids dropping  into the kernel unless it really
   has to. Since switching to the  kernel and back is quite expensive,
   doing so improves performance considerably.

   ---
   2.3.7 Monitors

   To make it  easier to write correct programs  some researchers have
   proposed   a  higher-level   synchronization  primitive   called  a
   *monitor*. A monitor is a  collection of procedures, variables, and
   data structures that are all grouped  together in a special kind of
   module or package.  Processes may call the procedures  in a monitor
   whenever  they  want  to,  but  they  cannot  directly  access  the
   monitor's internal data structures from procedures declared outside
   the monitor.

   C cannot be used here because monitors are a language concept and C
   does not have them.

   Monitors  have an  important property  that makes  them useful  for
   achieving mutual  exclusion: only  one process can  be active  in a
   monitor  at  any  instant.   Monitors  are  a  programming-language
   construct, so  the compiler knows  they are special and  can handle
   calls to monitor procedures differently from other procedure calls.
* HOLD [Economics][Book] Economics in one lesson                       :HOLD:
  - State "HOLD"       from "NEXT"       [2021-10-23 sáb 10:41] \\
    I need to do other stuff first
  :LOGBOOK:
  CLOCK: [2021-10-12 ter 09:47]--[2021-10-12 ter 10:26] =>  0:39
  CLOCK: [2021-10-12 ter 09:30]--[2021-10-12 ter 09:46] =>  0:16
  :END:
** 1 The lesson

   While  every group  has certain  economic interests  identical with
   those  of all  groups,  every  group has  also,  as  we shall  see,
   interests antagonistic to those of all other groups.

   In addition to these endless pleadings of self-interest, there is a
   second  main  factor  that  spawns  new  economic  fallacies  every
   day.  This is  the  persistent  tendency of  men  to  see only  the
   immediate  effects of  a given  policy, or  its effects  only on  a
   special group, and to neglect  to inquire what the long-run effects
   of that policy  will be not only  on that special group  but on all
   groups.

   *Lesson:* The  art of economics  consists in looking not  merely at
   the immeditate but  at the longer effects of any  act or policy; it
   consists in tracing the consequences  of that policy not merely for
   one group but for all groups.
* DONE [FP] A tutorial introduction to the lambda calculus
  CLOSED: [2021-10-16 sáb 20:50]
* CANCELLED [Course] Apostila contabilidade                       :CANCELLED:
  CLOSED: [2021-10-19 ter 21:57]
  - State "CANCELLED"  from              [2021-10-19 ter 21:57] \\
    Those notes are not very  good. They use the "jurisdiques" instead
  of clarity terms.  [[https://www.ginead.com.br/ava/curso/curso-online-gratis-contabilidade/apostila-contabilidade][Link do curso]]
** Conceito
  Contabilidade  é   a  ciência  que  estuda,   registra,  controla  e
  interpreta os fatos  ocorridos no patrimônio das  entidades com fins
  lucrativos ou não.
** Finalidades da contabilidade
   - Função administrativa: controlar o patrimônio
   - Função econômica: apurar o resultado
** Técnicas contábeis
   A  contabilidade  para  atingit   sua  finalidade  se  utiliza  das
   seguintes técnicas
*** Escrituração
    É o registro de todos os fatos que ocorrem no patrimônio.
*** Demonstrações financeiras
    São demonstrativos expositivos dos fatos ocorridos num determinado
    período. Representam a exposição gráfica dos fatos. São elas:

    - Balanço Patrimonial
    - Demonstração do Resultado do Exercício
    - Demonstração dos Lucros ou Prejuízos Acumulados
    - Demonstração das Mutações do patrimônio Líquido
    - Demonstração das Origens e Aplicações de Recursos
*** Auditoria
    É o  exame e a  verificação da  exatidão ou não  dos procedimentos
    contábeis.
*** Análise das Demonstrações Financeiras
    Analisa e interpreta as demonstrações financeiras.
* DONE [video][management] What you'be been taugh about management is wrong
  CLOSED: [2021-10-19 ter 22:25]

  [[https://www.youtube.com/watch?v=cA_WxUgIskk&ab_channel=USIEvents][Youtube video link]]

  Where are the references for the points presented?
  
** Wrong ideas
*** Be "fair"
*** Become "friend" with your team
*** "Team" is most important
    Individuals are more important
    
*** Manage young people/millennials differently
*** Asians/Indians/Americans are different
*** Tell your boss what you think about him

** The effective manager's 2 responsabilities
*** 1. Results
    Get the results that the company want.

    1 simple question: What your boss want from you?
    
*** 2. Keep your people
    Retention.

    Make  hard  to  get  hired  by the  company.  So  you  filter  the
    candidates.

** The 4 behaviours that matter
   The most important is to know the people that work to you. Each one
   at a time, not by the team.
   
*** You need to know your people
    One on ones
    
*** Talk about performance
    Feedback on how the employee is doing in a polite way
    
*** Ask for more
    Coaching
    
*** Push work down
    Delegation
* NEXT [FP] Lambda calculus
  :LOGBOOK:
  CLOCK: [2021-10-23 sáb 10:48]
  :END:
  :POST:
  
** What is lambda calculus

   According to [1], lambda calculus is a model of computation devised
   in the  1930s by  Alonzo Church,  where a calculus  is a  method of
   calculation  or reasoning;  the lambda  calculus is  a process  for
   formalizing a method.

   Basically,  lambda  calculus with  type  theory  is the  basis  for
   funcional programming. *REQUIRE MORE RESEARCH*
  
** What is functional programming (fp)

   Functional  programming  is a  paradigm  that  relies on  functions
   modeled on mathematical functions,  more specifically on the lambda
   calculus  logic.   The  essence  of  fp  is  that  programs  are  a
   combination  of expressions.  Expressions include  concrete values,
   variables, and also functions.

   Understanding functions as a mapping of a set of inputs to a set of
   outputs is crucial to understanding functional programming.

   One key feature  of fp languages is that  functions are first-class
   citizens (*REQUIRE MORE  RESEARCH*), so they can be  used as values
   or passed as arguments, or inputs, to yet more functions.

   Among the functional programming languages  available to be used we
   could  separate  those  in  ~pure~ languages  (i.e.:  Haskell)  and
   ~non-pure~ (i.e.: F# (*REQUIRE RESEARCH*)).

   The word ~pure~ in  fp is sometimes also used to  mean what is more
   properly    called    ~referential    transparency~.    Referential
   transparency means that the same function, given the same values to
   evaluate, will  always return  the same  result in  pure functional
   programming, as they do in math. [1]

   This can be achieved because fp avoid side effects since the values
   within the function are immutable.

   Pure languages like  Haskell lends to a high  degree os abstraction
   and composability.  Abstraction allows  you to write  shorter, more
   concise programs by factoring common, repeated structures into more
   generic code that can be reused. [1]
   
** Lambda calculus structure

   The lambda calculus has three  basic components, or ~lambda terms~:
   expressions, variables, and abstractions.

*** Expressions

    An  expression  can be  a  variable  name,  an abstraction,  or  a
    combination of those  things. The simplest expression  is a single
    variable. Variables here  have no meaning or value;  they are only
    names for potential inputs to functions.

*** Variables

*** Abstractions

    An abstraction is a function. It is  a lambda term that has a head
    (a lambda) and  a body and is applied to  an argument. An argument
    is an input value.

    Abstractions consist of two parts: the head and the body. The head
    of  the function  is a  \lambda  (lambda) followed  by a  variable
    name. The body of the function is another expression. So, a simple
    function might look like this:

    #+BEGIN_SRC latex
      \lambda x . x
    #+END_SRC

    The variable  named in  the head  is the  parameter and  binds all
    instances of that same variable in  the body of the function. That
    means, when we  apply this function to an argument,  each x in the
    body of the function will have the value of that argument.

    Lambda abstraction  `\lambda x . x`  differently from conventional
    functions  like `f(x)  =  x`  has no  name,  it  is an  ~anonymous
    function~. This is important because  it set boundaries to what we
    can do with this function. A  named function can be called by name
    by another function where an anonymous function cannot.

    In this structure we say that the x is a bound variable because it
    is the  parameter of  the head  of the lambda.  Where the  dot (.)
    separates the parameters of the lambda from the function body.

    The abstraction as a whole has no  name, but the reason we call it
    an abstraction  is that  it is  a generalization,  or abstraction,
    from a  concrete instance of  a problem, and it  abstracts through
    the introduction  of names. The  names stand for  concrete values,
    but  by using  named variables,  we allow  for the  possibility of
    applying  the general  function to  different values  (or, perhaps
    even values of different types).  When we apply the abstraction to
    arguments,   we  replace   the  names   with  values,   making  it
    concrete. [1]

** Alpha equivalence

    #+BEGIN_SRC latex
      \lambda x . x
    #+END_SRC

    The variable x  here is not semantically meaningful  except in its
    role in that single expression. Because of this, there's a form of
    equivalence between lambda terms called ~lambda equivalence~. This
    is a way of saying that:

    #+BEGIN_SRC latex
      \lambda x . x
      \lambda y . y
      \lambda z . z
    #+END_SRC

    all mean the same thing. They're all the same function. [1]

** Beta reduction

   When we  apply a function to  an argument, we substitute  the input
   expression for all instances of  bound variables within the body of
   the abstraction.  You also eliminate  the head of  the abstraction,
   since its  only purpose  was to  bind a  variable. This  process is
   called ~beta reduction~. [1]

   #+BEGIN_SRC latex
     (\lambda x . x) 2
     2
   #+END_SRC

   The only bound variable is a single x, so applying this function to
   2 returns 2. This function is  the ~identity~ function. All it does
   is accept a single argument x and return that same argument. [1]

   This is the same as the identity function in mathematical notation:
   `f(x) =  x`. One  difference is  that `f(x) =  x` is  a declaration
   involving  a function  named f  while the  lambda abstraction  is a
   function. [1]

   In other words, beta reduction is this process of applying a lambda
   term to an  argument, replacing the bound variables  with the value
   of the  argument, and  eliminating the  head. Eliminating  the head
   tells you the function has been applied. [1]

   HINT: Syntax [x := z] indicates  that z will be substituted for all
   occurrences of x.

   HINT:   Applications    in   the   lambda   calculus    are   ~left
   associative~.   That  is,   unless  specific   parentheses  suggest
   otherwise, they associate, or group, to the left.

   Example:

   #+BEGIN_SRC latex
     (\lambda x . x)(\lambda y . y) z
     =
     ((\lambda x . x)(\lambda y . y)) z
     (\lambda[x := (\lambda y . y)] . x)z
     (\lambda y . y) z
     \lambda [y := z] . y
     z
   #+END_SRC

*** Free variables

    The  purpose of  the head  of  the function  is to  tell us  which
    variables to replace when we apply  our function, that is, to bind
    the  variables.  A  bound  variable   must  have  the  same  value
    throughout the expression. [1]

    But sometimes the body expression has variables that are not named
    in the head. We call  those variables free variables. For example:
    [1]

    #+BEGIN_SRC latex
      \lambda x . xy
    #+END_SRC

    The x in the body is a bound variable and y is a free variable.

    Take a look at the following example: [1]

    #+BEGIN_SRC latex
      (\lambda x . xy) z
      (\lambda[x := z] . xy)
      zy
    #+END_SRC
   
** Multiple arguments

   Each lambda  can only bind  one parameter  and can only  accept one
   argument. Functions that require  multiple arguments have multiple,
   nested  head.  When you  apply  it  once  and eliminate  the  first
   (leftmost)  head,  the  next  one   is  applied  and  so  on.  This
   formulation was  originally discovered by Moses  Schonfinkel in the
   1920s but was later rediscovered  and named after Haskell Curry and
   is commonly called ~currying~. [1]

   #+BEGIN_SRC latex
     \lambda xy . xy
      = \lambda x . (\lambda y . y)
   #+END_SRC

   For example: [1]

   #+BEGIN_SRC latex
     \lambda xy . xy
     (\lambda xy . xy) 1 2
     (\lambda x . (\lambda y . xy)) 1 2
     (\lambda [x := 1] . (\lambda y . xy)) 2
     (\lambda y . 1y) 2
     (\lambda [y := 2] . 1y)
     1 2
   #+END_SRC

   That  wasn't  too interesting  because  it's  like nested  identity
   functions!  We can't  meaningfully  apply a  1 to  a  2. Let's  try
   something different: [1]

   #+BEGIN_SRC latex
     \lambda xy . xy
     (\lambda xy . xy)(\lambda z . a) 1
     (\lambda x . (\lambda y . xy))(\lambda z . a) 1
     (\lambda [x := (\lambda z . a)] . (\lambda y . xy)) 1
     (\lambda y . (\lambda z . a) y) 1
     (\lambda [y := 1] . (\lambda z . a) y)
     (\lambda z . a) 1
     (\lambda [z := 1] . a)
     a
   #+END_SRC

   The lambda calculus is a process or  method, like a game with a few
   simple rules for transforming lambdas, but no specific meaning. [1]
   
** Evaluation is simplification

   There are multiple  normal forms in lambda calculus,  but here when
   we refer  to normal form  we mean  ~beta normal form~.  Beta normal
   form is  when you cannot  beta reduce (apply lambdas  to arguments)
   the  terms  any further.  This  corresponds  to a  fully  evaluated
   expression, or, in  programming, a fully executed  program. This is
   important to know  so that you know when you're  done evaluating an
   expression. [1]

   Don't be intimidated  by calling the reduced form  of an expression
   its normal form. When  you want to say "2", do you  say 2000 / 1000
   each time or do you say 2?  The expression 2000 / 1000 is not fully
   evaluated because the division function has been fully applied (two
   arguments), so it  could be reduced, or evaluated.  In other words,
   there's a simpler form  it can be reduced to -  the number two. The
   normal form, therefore, is 2. [1]

   The point  is that if you  have a function, such  as (/), saturated
   (all arguments  applied) but you  haven't yet simplified it  to the
   final   result    then   it   is   not    fully   evaluated,   only
   applied.  Application   is  what   makes  evaluation/simplification
   possible. [1] 
   
** Combinators

   A combinator is a lambda  term with no free variables. Combinators,
   as the name suggests, serve only  to combine the arguments they are
   given. [1]

   So, the  following are combinators  because every term in  the body
   occurs in the head: [1]

   #+BEGIN_SRC latex
     \lambda x . x
     
     \lambda xy. x
     
     \lambda xyz . xz(yz)
   #+END_SRC

   The point of combinators is that they are a special class of lambda
   expressions  that  can only  combine  the  arguments it  is  given,
   without injecting any new values or random data. [1]
   
** Divergence

   Not  all reducible  lambda terms  reduce  neatly to  a beta  normal
   form. This isn't because they're  already fully reduced, but rather
   because  they diverge.  Divergence  here means  that the  reduction
   process never terminates or ends. [1]

   Here's an  example of a  lambda term called ~omega~  that diverges:
   [1]

   #+BEGIN_SRC latex
     (\lambda x . xx)(\lambda x . xx)
     (\lambda [x := (\lambda x . xx)] . xx)
     (\lambda x . xx)(\lambda x . xx)
   #+END_SRC

   This matters  in programming because  terms that diverge  are terms
   that don't  produce an  answer or meaningful  result. Understanding
   what  will  terminate means  understanding  what  programs will  do
   usefull work and return the answer we want. [1]
   
** Church encoding

   In mathematics, Church encoding is a means of representing data and
   operators  in  the  lambda  calculus. The  Church  numerals  are  a
   representation of  the natural  numbers using lambda  notation. The
   method is  named for Alonzo Church,  who first encoded data  in the
   lambda calculus this way. [2]

   Terms  that are  usually  considered primitive  in other  notations
   (such as integers,  booleans, pairs, lists, and  tagged unions) are
   mapped to higher-order functions under Church encoding. [2]

*** Church numerals

    Church numerals  are the representations of  natural numbers under
    Church encoding. The higher-order function that represents natural
    number n  is a  function that  maps any function  f to  its n-fold
    composition.  In simpler  terms,  the "value"  of  the numeral  is
    equivalent to  the number of  times the function  encapsulates its
    argument. [2]

    All Church numerals are functions that take two parameters. [2]

    | Number | Lambda expression                      |
    |      0 | \lambda f . \lambda x . x              |
    |      1 | \lambda f . \lambda x . fx             |
    |      2 | \lambda f . \lambda x . f(fx)          |
    |      3 | \lambda f . \lambda x . f(f(fx)        |
    |    ... | ...                                    |
    |      n | \lambda f . \lambda x . f^{\circle n}x |

    The Church numeral  3 represents the action of  applying any given
    function three  times to a  value. [...] The function  itself, and
    not its end result, is the Church numeral 3. [2]
   
** References

   [1] - [Book] Haskell programming from first principles
   
   [2] - [[https://en.wikipedia.org/wiki/Church_encoding#Church_numerals][[Link] Church encoding]]
   
