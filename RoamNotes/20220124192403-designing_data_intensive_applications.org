:PROPERTIES:
:ID:       61e93da7-245e-4b05-a68b-29140f926091
:END:
#+title: [Book] Designing Data-Intensive Applications

This org file will  be used to store some notes regarding  the book mentioned in
the title.  Btw, my idea  is to actually develop  a system concomitantly  to put
those lessons in practice. Later I'll describe this project in more detail.

#+BEGIN_QUOTE

  Technology  is  a   powerful  force  in  our  society.   Data,  software,  and
  communication can  be used for  bad: to  entrench unfair power  structures, to
  undermine human rights, and to protect  vested interests. But they can also be
  used  for good:  to make  underrepresented  people's voices  heard, to  create
  opportunities for everyone, and to avert disasters.  This book is dedicated to
  everyone working toward the good.

#+END_QUOTE

* Foundations of data systems

+ *Reliability:*

  Tolerating hardware & software faults and human errors.

  The  system  should  continue  to work  /correctly/  (performing  the  correct
  function at the desired level of  performance) even in the face of /adversity/
  (hardware or software faults, and even human error).
  
+ *Scalability:*

  Measuring load & performance. Latency, percentiles, throughput.

  As the  system grows (in  data volume,  traffic volume, or  complexity), there
  should be reasonable ways of dealing with that growth.
  
+ *Maintainability:*

  Operability, simplicity & evolvability.

  Over time,  many different  people will  work on  the system  (engineering and
  operations, both maintaining  current behavior and adapting the  system to new
  use cases), and they should all be able to work on it /productively/.

Many    applications    today    are    /data-intensive/,    as    opposed    to
/compute-intensive/.  Raw  CPU power  is  rarely  a  limiting factor  for  these
applications - bigger problems are usually the amount of data, the complexity of
data, and the speed at which it is changing.

---
[...] How do we  make our systems reliable, in spite  of unreliable humans?  The
best systems combine several approaches:

- Design systems in a way that minimizes opportunities for error.

- Decouple the places where people make the most mistakes from the places they
  can cause failures (provide a /sandbox/ environment).

- Test thoroughly at all levels, from unit tests to whole-system integration tests
  and manual tests.

- Allow quick and easy recovery from human errors, to minimize the impact in the
  case of a failure.

- Set up detailed and clear monitoring, such as performance metrics and error rates.
  In other engineering disciplines this is referred to as /telemetry/.

- Implement good management practices and training.

---
[...] An architecture that  is appropriate for one level of  load is unlikely to
cope with 10 times  that load. If you are working on  a fast-growing service, it
is therefore  likely that you  will need to  rethink your architecture  on every
order of magnitude load increase —or perhaps even more often than that.

---
[...]   In an  early-stage  startup or  an unproven  product  it’s usually  more
important to be able to iterate quickly  on product features than it is to scale
to some hypothetical future load.

* Data models and query languages

In this chapter we will look at  a range of general-purpose data models for data
storage  and querying.  In particular,  we  will compare  relational model,  the
document model, and a few graph-based data  models. We will also look at various
query languages and compare their use cases.

*** The birth of NoSQL

There  are  several driving  forces  behind  the  adoption of  NoSQL  databases,
including:

- A need for  greater scalability than relational databases  can easily achieve,
  including very large datasets or very high write throughput;

- A  widespread preference  for free  and open  source software  over commercial
  database products;

- Specialized query  operations that  are not well  supported by  the relational
  model;

- Frustration with the restrictiveness of relational schemas, and a desire for a
  more dynamic and expressive data model.

*** The object-relational mismatch

Most  applications  development today  is  done  in object-oriented  programming
languages, which leads to  a common criticism of the SQL data  model: if data is
stored in  relational tables, an  awkward translation layer is  required between
the objects in the application code and  the database model of tables, rows, and
columns. The  disconnect between  the models is  sometimes called  an *impedance
mismatch*.

ORM,  Object-relational mapping,  frameworks are  used to  reduce the  amount of
boilerplate required for this translation  layer, but they can't completely hide
the differences between the two models.

*** Normalization

Literature on the relational model distinguishes several different normal forms,
but the distinctions  are of little practical  interest.

*As a rule of  thumb, if you're duplicating values that could  be stored in just
one place, the schema is not normalized*.

*** MapReduce querying

/MapReduce/ is a programming model for  processing large amounts of data in bulk
across many  machines, popularized  by Google.  A limited  form of  MapReduce is
supported  by  some  NoSQL  datastores,  including MongoDB  and  CouchDB,  as  a
mechanism for performing read-only queries across many documents.

** Graph-like data models

There are several different, but related, ways of structuring and querying data
in graphs. In this sections we will discuss the /property graph/ model
(implemented by Neo4j, Titan, and InfiniteGraph) and the /triple-store/ model
(implemented by Datomic, AllegroGraph, and others).

*** Property graphs

In a property graph model, each vertex consists of:

+ A unique identifier
+ A set of outgoing edges
+ A set of incoming edges
+ A collection of properties (key-value pairs)

Each edge consists of:

+ A unique identifier
+ The vertex at which the edge starts (the /tail/ vertex)
+ The vertex at which the edge ends (the /head/ vertex)
+ A label to describe the kind of relationship between the two vertices
+ A collection of properties (key-value pairs)

[...] Graphs are good for evolvability: as you add features to your application,
a graph can easily be extended to accommodate changes in your application's data
structures.

** Summary

1. /Document databases/ target use cases where data comes in self-contained
   documents and relationships between one document and another are rare.
2. /Graph databases/ go in the opposite direction, targeting use cases where
   anything is potentially related to everything.

One thing that document and graph databases have in common is that they
typically don't enforce a schema for the data they store, which can make it
easier to adapt applications to changing requirements. However, your application
most likely still assumes that data has a certain structure; it's just a
question of whether the schema is explicit (enforced on write) or implicit
(handled on read).

* Storage and retrieval

On the most fundamental level, a database needs to do two things: when you give
it some data, it should store the data, and when you ask it again later, it
should give the data back to you.

Why should you, as an application developer, care how the database handles
storage and retrieval internally? You're probably not going to implement your
own storage engine from scratch, but you do need to select a storage engine that
is appropriate for your application, from the many that are available. In order
to tune a storage engine to perform well on your kind of workload, you need to
have a rough idea of what the storage engine is doing under the hood.

---
Definition of /log/:

The word /log/ is often used to refer to application logs, where an application
outputs text that describes what's happening. In this book, /log/ is used in the
more general sense: an append-only sequence of records. It doesn't have to be
human-readable; it might be binary and intended only for other programs to read.

---

** Comparing B-Trees and LSM-Trees

Even though B-tree implementations are generally more mature than LSM-tree
implementations, LSM-trees are also interesting due to their performance
characteristics. As a rule of thumb, LSM-trees are typically faster for writes,
whereas B-trees are thought to be faster for reads. Reads are typically slower
on LSM-trees because they have to check several different data structures and
SSTables at different stages of compaction.

** Summary

On a high level, we saw that storage engines fall into two broad categories:
those optimized for transaction processing (OLTP), and those optimized for
analytics (OLAP). 
