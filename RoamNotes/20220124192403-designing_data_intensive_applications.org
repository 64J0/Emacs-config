:PROPERTIES:
:ID:       61e93da7-245e-4b05-a68b-29140f926091
:END:
#+title: [Book] Designing Data-Intensive Applications

This org file will  be used to store some notes regarding  the book mentioned in
the title.  Btw, my idea  is to actually develop  a system concomitantly  to put
those lessons in practice. Later I'll describe this project in more detail.

#+BEGIN_QUOTE

  Technology  is  a   powerful  force  in  our  society.   Data,  software,  and
  communication can  be used for  bad: to  entrench unfair power  structures, to
  undermine human rights, and to protect  vested interests. But they can also be
  used  for good:  to make  underrepresented  people's voices  heard, to  create
  opportunities for everyone, and to avert disasters.  This book is dedicated to
  everyone working toward the good.

#+END_QUOTE

* Foundations of data systems

+ *Reliability:*
  Tolerating hardware & software faults and human errors.

  The  system  should  continue  to work  /correctly/  (performing  the  correct
  function at the desired level of  performance) even in the face of /adversity/
  (hardware or software faults, and even human error).
  
+ *Scalability:*
  Measuring load & performance. Latency, percentiles, throughput.

  As the  system grows (in  data volume,  traffic volume, or  complexity), there
  should be reasonable ways of dealing with that growth.
  
+ *Maintainability:*
  Operability, simplicity & evolvability.

  Over time,  many different  people will  work on  the system  (engineering and
  operations, both maintaining  current behavior and adapting the  system to new
  use cases), and they should all be able to work on it /productively/.

Many    applications    today    are    /data-intensive/,    as    opposed    to
/compute-intensive/.  Raw  CPU power  is  rarely  a  limiting factor  for  these
applications - bigger problems are usually the amount of data, the complexity of
data, and the speed at which it is changing.

---
[...] How do we  make our systems reliable, in spite  of unreliable humans?  The
best systems combine several approaches:

- Design systems in a way that minimizes opportunities for error.

- Decouple the places where people make the most mistakes from the places they
  can cause failures (provide a /sandbox/ environment).

- Test thoroughly at all levels, from unit tests to whole-system integration tests
  ans manual tests.

- Allow quick and easy recovery from human errors, to minimize the impact in the
  case of a failure.

- Set up detailed and clear monitoring, such as performance metrics and error rates.
  In other engineering disciplines this is referred to as /telemetry/.

- Implement good management practices and training.

---
[...] An architecture that  is appropriate for one level of  load is unlikely to
cope with 10 times  that load. If you are working on  a fast-growing service, it
is therefore  likely that you  will need to  rethink your architecture  on every
order of magnitude load increase —or perhaps even more often than that.

---
[...]  In an  early-stage startup  or an  unpro‐ ven  product it’s  usually more
important to  be able to  iterate quickly  on product fea‐  tures than it  is to
scale to some hypothetical future load.

* Data models and query languages

In this chapter we will look at  a range of general-purpose data models for data
storage  and querying.  In particular,  we  will compare  relational model,  the
document model, and a few graph-based data  models. We will also look at various
query languages and compare their use cases.

*** The birth of NoSQL

There  are  several driving  forces  behind  the  adoption of  NoSQL  databases,
including:

- A need for  greater scalability than relational databases  can easily achieve,
  including very large datasets or very high write throughput;

- A  widespread preference  for free  and open  source software  over commercial
  database products;

- Specialized query  operations that  are not well  supported by  the relational
  model;

- Frustration with the restrictiveness of relational schemas, and a desire for a
  more dynamic and expressive data model.

*** The object-relational mismatch

Most  applications  development today  is  done  in object-oriented  programming
languages, which leads to  a common criticism of the SQL data  model: if data is
stored in  relational tables, an  awkward translation layer is  required between
the objects in the application code and  the database model of tables, rows, and
columns. The  disconnect between  the models is  sometimes called  an *impedance
mismatch*.

ORM,  Object-relational mapping,  frameworks are  used to  reduce the  amount of
boilerplate required for this translation  layer, but they can't completely hide
the differences between the two models.

*** Normalization

Literature on the relational model distinguishes several different normal forms,
but the distinctions  are of little practical  interest.

*As a rule of  thumb, if you're duplicating values that could  be stored in just
one place, the schema is not normalized*.
