:PROPERTIES:
:ID:       61e93da7-245e-4b05-a68b-29140f926091
:END:
#+title: [Book] Designing Data-Intensive Applications

This org file will  be used to store some notes regarding  the book mentioned in
the title.  Btw, my idea  is to actually develop  a system concomitantly  to put
those lessons in practice. Later I'll describe this project in more detail.

#+BEGIN_QUOTE

  Technology  is  a   powerful  force  in  our  society.   Data,  software,  and
  communication can  be used for  bad: to  entrench unfair power  structures, to
  undermine human rights, and to protect  vested interests. But they can also be
  used  for good:  to make  underrepresented  people's voices  heard, to  create
  opportunities for everyone, and to avert disasters.  This book is dedicated to
  everyone working toward the good.

#+END_QUOTE

* Foundations of data systems

+ *Reliability:*

  Tolerating hardware & software faults and human errors.

  The  system  should  continue  to work  /correctly/  (performing  the  correct
  function at the desired level of  performance) even in the face of /adversity/
  (hardware or software faults, and even human error).
  
+ *Scalability:*

  Measuring load & performance. Latency, percentiles, throughput.

  As the  system grows (in  data volume,  traffic volume, or  complexity), there
  should be reasonable ways of dealing with that growth.
  
+ *Maintainability:*

  Operability, simplicity & evolvability.

  Over time,  many different  people will  work on  the system  (engineering and
  operations, both maintaining  current behavior and adapting the  system to new
  use cases), and they should all be able to work on it /productively/.

Many    applications    today    are    /data-intensive/,    as    opposed    to
/compute-intensive/.  Raw  CPU power  is  rarely  a  limiting factor  for  these
applications - bigger problems are usually the amount of data, the complexity of
data, and the speed at which it is changing.

---
[...] How do we  make our systems reliable, in spite  of unreliable humans?  The
best systems combine several approaches:

- Design systems in a way that minimizes opportunities for error.

- Decouple the places where people make the most mistakes from the places they
  can cause failures (provide a /sandbox/ environment).

- Test thoroughly at all levels, from unit tests to whole-system integration tests
  and manual tests.

- Allow quick and easy recovery from human errors, to minimize the impact in the
  case of a failure.

- Set up detailed and clear monitoring, such as performance metrics and error rates.
  In other engineering disciplines this is referred to as /telemetry/.

- Implement good management practices and training.

---
[...] An architecture that  is appropriate for one level of  load is unlikely to
cope with 10 times  that load. If you are working on  a fast-growing service, it
is therefore  likely that you  will need to  rethink your architecture  on every
order of magnitude load increase —or perhaps even more often than that.

---
[...]   In an  early-stage  startup or  an unproven  product  it’s usually  more
important to be able to iterate quickly  on product features than it is to scale
to some hypothetical future load.

* Data models and query languages

In this chapter we will look at  a range of general-purpose data models for data
storage  and querying.  In particular,  we  will compare  relational model,  the
document model, and a few graph-based data  models. We will also look at various
query languages and compare their use cases.

*** The birth of NoSQL

There  are  several driving  forces  behind  the  adoption of  NoSQL  databases,
including:

- A need for  greater scalability than relational databases  can easily achieve,
  including very large datasets or very high write throughput;

- A  widespread preference  for free  and open  source software  over commercial
  database products;

- Specialized query  operations that  are not well  supported by  the relational
  model;

- Frustration with the restrictiveness of relational schemas, and a desire for a
  more dynamic and expressive data model.

*** The object-relational mismatch

Most  applications  development today  is  done  in object-oriented  programming
languages, which leads to  a common criticism of the SQL data  model: if data is
stored in  relational tables, an  awkward translation layer is  required between
the objects in the application code and  the database model of tables, rows, and
columns. The  disconnect between  the models is  sometimes called  an *impedance
mismatch*.

ORM,  Object-relational mapping,  frameworks are  used to  reduce the  amount of
boilerplate required for this translation  layer, but they can't completely hide
the differences between the two models.

*** Normalization

Literature on the relational model distinguishes several different normal forms,
but the distinctions  are of little practical  interest.

*As a rule of  thumb, if you're duplicating values that could  be stored in just
one place, the schema is not normalized*.

*** MapReduce querying

/MapReduce/ is a programming model for  processing large amounts of data in bulk
across many  machines, popularized  by Google.  A limited  form of  MapReduce is
supported  by  some  NoSQL  datastores,  including MongoDB  and  CouchDB,  as  a
mechanism for performing read-only queries across many documents.

** Graph-like data models

There are several different, but related, ways of structuring and querying data
in graphs. In this sections we will discuss the /property graph/ model
(implemented by Neo4j, Titan, and InfiniteGraph) and the /triple-store/ model
(implemented by Datomic, AllegroGraph, and others).

*** Property graphs

In a property graph model, each vertex consists of:

+ A unique identifier
+ A set of outgoing edges
+ A set of incoming edges
+ A collection of properties (key-value pairs)

Each edge consists of:

+ A unique identifier
+ The vertex at which the edge starts (the /tail/ vertex)
+ The vertex at which the edge ends (the /head/ vertex)
+ A label to describe the kind of relationship between the two vertices
+ A collection of properties (key-value pairs)

[...] Graphs are good for evolvability: as you add features to your application,
a graph can easily be extended to accommodate changes in your application's data
structures.

** Summary

1. /Document databases/ target use cases where data comes in self-contained
   documents and relationships between one document and another are rare.
2. /Graph databases/ go in the opposite direction, targeting use cases where
   anything is potentially related to everything.

One thing that document and graph databases have in common is that they
typically don't enforce a schema for the data they store, which can make it
easier to adapt applications to changing requirements. However, your application
most likely still assumes that data has a certain structure; it's just a
question of whether the schema is explicit (enforced on write) or implicit
(handled on read).

* Storage and retrieval

On the most fundamental level, a database needs to do two things: when you give
it some data, it should store the data, and when you ask it again later, it
should give the data back to you.

Why should you, as an application developer, care how the database handles
storage and retrieval internally? You're probably not going to implement your
own storage engine from scratch, but you do need to select a storage engine that
is appropriate for your application, from the many that are available. In order
to tune a storage engine to perform well on your kind of workload, you need to
have a rough idea of what the storage engine is doing under the hood.

---
Definition of /log/:

The word /log/ is often used to refer to application logs, where an application
outputs text that describes what's happening. In this book, /log/ is used in the
more general sense: an append-only sequence of records. It doesn't have to be
human-readable; it might be binary and intended only for other programs to read.

---

** Comparing B-Trees and LSM-Trees

Even though B-tree implementations are generally more mature than LSM-tree
implementations, LSM-trees are also interesting due to their performance
characteristics. As a rule of thumb, LSM-trees are typically faster for writes,
whereas B-trees are thought to be faster for reads. Reads are typically slower
on LSM-trees because they have to check several different data structures and
SSTables at different stages of compaction.

** Summary

On a high level, we saw that storage engines fall into two broad categories:
those optimized for transaction processing (OLTP), and those optimized for
analytics (OLAP). 

* Encoding and evolution

In this chapter we will look at several formats for encoding data, including
JSON, XML, Protocol Buffers, Thrift, and Avro. In particular, we will look at
how they handle schema changes and how they support systems where old and new
data and code need to coexist. We will then discuss how those formats are used
for data storage and for communication: in web services, Representational State
Transfer (REST), and remote procedure calls (RPC), as well as message-passing
systems such as actors and message queues.

** Formats of encoding data

Programs usually work with data in (at least) two different representations:

+ *In memory* (objects, structs, lists, arrays, hash tables, trees, etc). This
  data structures are optimized for efficient acces and manipulation by the
  CPU (typically using pointers);

+ *Encoded data* (when you want to write data to a file or sent it over the
  network). You need to encode this data in some kind of self-contained sequence
  of bytes (JSON for example).

  The translation from the in-memory representation to a byte sequence is called
  /encoding/ (also known as /serialization/ or /marshalling/), and the reverse
  is called /decoding/ (/parsing/, /deserialization/, /unmarshalling/).

** Language-specific encoding formats

Many programming languages come with built-in support for encoding in-memory
objects into byte sequences.

+ Java -> java.io.Serializable
+ Ruby -> Marshal
+ Python -> pickle

These encoding libraries are very convenient, because they allow in-memory
objects to be saved and restored with minimal additional code.

*** Deep problems in language-specific encoding

The encoding is often tied to a particular programming language, and reading the
data in another language is very difficult.

In order to restore data in the same object types, the decoding process needs to
be able to instantiate arbitrary classes. This is frequently a source of
security problems: if an attacker can get your application to decode an
arbitrary byte sequence, they can instantiate arbitrary classes, which in turn
often allows them to do terrible things such as remotely executing arbitrary
code.

Versioning data is often an afterthought in these libraries (inconvenient
problems of forward and backward compatibility).

Efficiency (CPU time taken to encode or decode, and the size of the encoded
structure) is also often an afterthought.

** JSON, XML and binary variants

- Standardized encodings.

** Thrift and protocol buffers

Apache Thrift (made by Facebook) and Protocol buffers (aka protobuf, made by
Google) are binary encoding libraries that are based on the same principle.

Both technologies require a schema for any data that is encoded.

#+begin_src json
  {
    "username": "Martin",
    "favoritenumber": 1337,
    "intersts": ["daydreaming", "hacking"]
  }
#+end_src

#+begin_src thrift
  struct Person {
    1: required string       userName,
    2: optional i64          favoriteNumber,
    3: optional list<string> interests
  }
#+end_src

#+begin_src protobuf
  message Person {
    required string userName      = 1;
    optional int64 favoriteNumber = 2;
    repeated string interests     = 3;
  }
#+end_src

One detail to note: in these schemas, each field was marked either required or
optional, but this makes no difference to how the field is encoded (nothing in
the binary data indicates whether a field was required). The difference is
simply that required enables a runtime check that fails if the field is not set,
which can be useful for catching bugs.

*** Backward compatibility

Those data encoding formats are organized by their tag number. Removing a field
is just like adding a field, with backward and forward compatibility concerns
reversed. That means you can only remove a field that is optional (a required
field can never be removed), and you can never use the same tag number again
(because you may still have data written somewhere that includes the old tag
number, and that field must be ignored by new code).

*** Changing datatype of a field

What about changing the datatype of a field? That may be possible - check the
documentation for details - but there is a risk that values will lose precision
or get truncated.

**** Example:

Previous data field -> 32-bit integer
New data field      -> 64-bit integer

Parser can fill in any missing bits with zero.

---

Previous data field -> 64-bit integer
New data field      -> 32-bit integer

If the decoded 64-bit value won't fit in 32-bits, it will be truncated.

*** Protobuf array

A curous detail of Protocol Buffers is that it does not have a list of array
datatype, but instead has a repeated marker for fields (which is a third option
alongside required and optional).

** Avro

Apache Avro is another binary encoding format that is interestingly different
from Protocol Buffers and Thrift. It was started in 2009 as a subproject of
Hadoop, as a result of Thrift not being a good fit for Hadoop's use cases.

Avro also uses a schema to specify the structure of the data being encoded. It
has two schema languages: one (Avro IDL) intended for human editing, and one
(based on JSON) that is more easily machine readable.

#+BEGIN_SRC avro-idl
  record Person {
    string               userName;
    union { null, long } favoriteNumber = null;
    array<string>        interests;
  }
#+END_SRC

To parse the binary data, you go through the fields in the order that they
appear in the schema and use the schema to tell you the datatype of each
field. This means that the binary data can only be decoded correctly if the code
reading the data is using the /exact same schema/ as the code that wrote the
data. Any mismatch in the schema between the reader and the writer would mean
incorrectly decoded data.

The key idea with Avro is that the writer's schema and the reader's schema don't
have to be the same - they only need to be compatible.

** Code generation and dynamically typed languages

Thrift and Protocol Buffers rely on code generation: after a schema has been
defined, you can generate code that implements this schema in a programming
language of your choice. This is useful in statically typed languages such as
Java, C++, or C#, because it allows efficient in-memory structures to be used
for decoded data, and it allows type checking and autocompletion in IDEs when
writing programs that access the data structures.

In dynamically typed programming languages such as JavaScript, Ruby or Python,
there is not much point in generating code, since there is no compile-time type
checker to satisfy.

** Message brokers

One process sends a message to a named queue or topic, and the broker ensures
that the message is delivered to one or more consumers of or subscribers to that
queue or topic. There can be many producers and many consumers on the same
topic.
