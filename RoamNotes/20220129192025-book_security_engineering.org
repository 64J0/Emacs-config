:PROPERTIES:
:ID:       f009f5b6-ec4d-4449-b91e-54397f3ab822
:END:
#+title: [Book] Security Engineering
#+bibliography: "../bib/security-engineering-3rd.bib"

[[https://github.com/64J0/Emacs-config/blob/master/org/security-engineering-3rd-book.org][Repository on github]].

* Chapter 6 - Access Control

[[id:273edbf4-0d24-45f6-bd13-a8fadfbb6a15][Access Control]]

* Chapter 7 - Distributed Systems

[[id:8994353b-0aaf-441f-b88d-ae46f37714f0][[Security] Distributed systems]]

* Chapter 8 - Economics

[[id:7972dc99-50c7-4c2e-946c-df7fa41c3154][[Security] Economics]]

* Chapter 9 - Multilevel Security

[[id:6cc42aac-f451-4f5d-bcf6-c1d33c0d0118][[Security] Multilevel security]]

* Chapter 10 - Boundaries

[[id:7f7ccf26-37e9-46a5-aa0c-b5fcd38fb4fd][[Security] Boundaries]]

* Chapter 11 - Inference Control

  Just as Big Tobacco spent decades denying that smoking causes lung cancer, and
  Big Oil spent decades denying climate change, so also Big Data has spent
  decades pretending that sensitive personal data can easily be 'anonymised' so
  it can be used as an industrial raw material without infriging on the privacy
  rights of the data subjects.

  Anonymisation is an aspirational term that means stripping identifying
  information from data in such a way that useful statistical research can be
  done without leaking information about identifiable data subjects.

  [...]

  Since 2006, we have a solid theory of exactly how much protection we can get
  from adding randomness: differential privacy.

  [...]

  By 2011 Google was describing its core competence as 'the statistical data
  mining of crowdsourced data'; as the datasets got larger, and basic
  statistical techniques were augmented with machine learning, the amount we can
  learn has grown.

  [...]

  So is it possible to do anonymisation properly? The answer is yes; in certain
  circumstances, it is. Although it is not possible to create anonymous datasets
  that can be used to answer any question, we can sometimes provide a dependable
  measure of privacy when we set out to answer a specific set of reasearch
  questions. This brings us to the theory of differential privacy.

** 11.3 - Differential privacy

   In 2006, Cynthia Dwork, Frank McSherry, Kobbi Nissim and Adam Smith published
   a seminal paper showing how you could systematically analyse privacy systems
   that added noise to prevent disclosure of sensitive statistics in a
   database. (“Calibrating noise to sensitivity in private data analysis”, Third
   conference on Theory of Cryptography (2006)). Their theory, /differential
   privacy/, enables the security engineer to limit the probability of
   disclosure, even in the presence of an adversary with unbounded computational
   power and copious side information, and can thus be seen as the equivalent of
   the one-time pad and unconditionally secure authentication codes in
   cryptography.

   + Gold standard for both statistical database security and for anonymisation
     in general.

   [...]

   There is now a growing research literature exploring how such mechanisms can
   be extended for static to dynamic databases, to data streams, to mechanism
   design and to machine learning. But can the promise of learning nothing
   useful about individuals while learning useful information about a
   population, be realised in practical applications?

   Differential privacy is now getting a full-scale test in the 2020 US
   census. The census is not allowed to publish anything that identifies the
   data of any individual or establishment; collected data must by law be kept
   confidential for 72 years and used only for statistical purposes until then.

** 11.4 - Mind the gap?

   Firms want to be responsible, but how do you give live data to your
   development and test teams? How can you collaborate with academics and
   startups? How can you sell data products? Anonymisation technologyu is all
   pretty rudimentary at this scale, and as you just don't know what's goingo
   on, it's beyong the scope of differential privacy or anything else you can
   analyse cleanly. You can tokenise the data on ingest to get rid of the
   obvious names, then control access and use special tricks for time series and
   location streams, but noise addition doesn't work on trajectories and there
   are lots of creative ways to re-identify location data (e.g., photos of
   celebs getting in and out of taxis). This get even harder where people are
   partially authorised and have partial access.

   Future problems may come from AI and machine learning; that's the fashion
   now, following the 'Big Data' fashion of the mid-2010s that led firms to set
   up large data warehouses. You're now training up systems that generally can't
   explain what they do, on data you don't really understand. We already know of
   lots of things that can go wrong. Insurance systems jack up premiums in
   minority neighbourhoods, breaking anti-discrimination laws. And machine
   learning systems inhale existing social prejudices along with their training
   data; as machine-translation systems read gigabytes of online text, they
   become much better at translation but they also become racist, sexist and
   homophobic. Another problem is that if a neural network is trained on
   personal data, then it will often be able to identify some of those persons
   if it comes across them again - so you can't just train it and then release
   it in the hope that its knowledge is somehow anonymous, as we might hope for
   averages derived from large aggregates of data.

* Chapter 12 - Banking and Bookkeeping

  #+begin_comment
    bookkeeping = contabilidade.
  #+end_comment
  
  A banking system should prevent customers from cheating on each other, or the
  bank; it should prevent bank staff from cheating the bank, or its customers;
  and the evidence it provides should be good enough that none of them can get
  away with falsely accusing others of cheating. Banking and bookkeeping
  pioneered the use of dual control, also known nowadays as multi-party
  authorisation. [...] This kind of system gives us a well-understood model of
  protection in which confidentiality plays little role, but where the integrity
  of records (and their immutability once made) is paramount.

*** Double-entry bookkeeping

    The idea behind double-entry bookkeeping is simple: each transaction is
    posted to two separate books, as a credit in one and a debit in the
    other. [...] At the end of the day, the books should /balance/, that is, add
    up to zero; the assets and the liabilities should be equal. In all but the
    smallest firms, the books were kept by different clerks.

    We arrange things so that each branch can be balanced separately. Each
    cashier will balance their cash tray before locking it in the vault
    overnight; the debits in the cash ledger should exactly balance the physical
    banknotes they've collected. So most frauds need the collusion of two or
    more people, and this principle of split responsibility, also known as dual
    control or multi-party authorisation (MPA), is complemented by audit. Not
    only are the books audited at year end, but there are random audits too;
    inspectors may descend on a branch at no notice and insist that all the
    books are balanced before the staff go home.

*** The Clark-Wilson security policy model

    In this model, some data items are constrained so that they can only be
    acted on by a certain set of transformation procedures.

    + UDI: Unconstrained data item
    + CDI: Constrained data item
    + IVP: Integrity verification procedures
    + TP: Transformation procedures

    ---
    
    1. The system will have an IVP for validating the integrity of any CDI;
    2. The application of a TP to any CDI must maintain its integrity;
    3. A CDI can only be changed by a TP;
    4. Subjects can only initiate certain TPs on certain CDIs;
    5. Triples must enforce an appropriate separatio-of-duty policy on
       subjects;
    6. Certain special TPs on UDIs can produce CDIs as output;
    7. Each application of a TP must cause enough information to reconstruct
       it to be written to a special append-only CDI;
    8. The system must authenticate subjects attempting to initiate a TP;
    9. The system must let only special subjects (i.e., security officers)
       make changes to authorization-related lists.

    [...]

    The hard question remains, namely: how do we control the risks from
    dishonest staff? (rule 5).

    [...]
    
    What happens in practice is that the big four accountancy firms have a list
    of controls that they push to their audit clients - a typical company may
    have a checklist of about 300 internal controls that it has to maintain,
    depending on what sector it's in.

*** Designing internal controls

    [...] Self-regulation failed to stop the excesses of the dotcom era, and
    following the collapse of Enron there was intervention from US lawmakers in
    the form of the /Sarbanes-Oxley Act/ (SOX) of 2002. SOX regulates all US
    public companies, making senior executives responsible for the accuracy and
    completeness of financial reports, whose truthfulness CEOs have to certify;
    protecting whistleblowers, who are the main source of information on insider
    fraud; and making managers responsible for maintaining "adequate internal
    control structure and procedures for financial reporting". It also demands
    that auditors disclose any "material weaknesses". Most of the compliance
    cost of SOX are reckoned to come from internal controls. 

*** Finding the weak spots

    If you are ever responsible for security in an organisation, you should not
    just think about which components might, by their failure, cause a bad
    enough loss to make a material difference to the bottom line. You need to
    think about the people too, and their external relationships.

    + Which of your managers could defraud your company by colluding with
      customers or suppliers?
    + Could a branch manager be lending money to a dodgy business run by his
      cousin against forged collateral?
    + Could he have sold life-insurance policies to nonexistent people and
      forged their death certificates?
    + Could an operations manager be taking bribes from a supplier?
    + Could your call-centre staff be selling data from the accounts they've
      dealt with to a phishing gang who use this data to impersonate your
      company to your customers?

    ---
    
    Lessons:
    
    According to statistical studies, 1% of staff fall into temptation every
    year.

    A trusted person is one who can damage you.
      
    ---

** Credit cards

   [...]

   When you use a credit card to pay for a purchase in a store, the transaction
   flows from the merchant to their bank (the acquiring bank), which pays them
   after deducting a merchant discount of typically just under 2% for a small
   merchant. If the card was issued by a different bank, the transaction now
   flows to a switch such as VISA, which passes it to the issuing bank for
   payment. Each transaction involves two components: authorisation, when you
   present your card at a merchant and they want to know right now whether to
   give you the goods, and settlement, which flows through a separate system and
   gets money to the merchant, often two or three days later. The issuer also
   gets a slice of the merchant discount, but makes most of its money from
   extending credit to cardholders.
   
*** Fraud engines

    [...]

    The core of a good fraud engine tends to be several dozen signals extracted
    from the transaction stream on the basis of a set of well-understood threat
    vectors (such as bad IP addresses, or too many logons from the same IP
    address) and a set of quality signals (such as 'card old but good'). These
    signals are then fed to a machine-learning system that scores the
    transactions. The signals appear to be the most important part of the
    design, not whether you use an SVM or a Bayesian network. The signals need
    to be continuously curated and updated as the bad guys learn new tricks, and
    the fraud engine needs to be well integrated with the human processes. As
    for how engines fail, the regulator's report into a 2016 fraud against Tesco
    Bank found that the staff failed to 'exercise due skill, case and
    dilligence' over the fraud detection rules, and to 'respond to the attack
    with sufficient rigor, skill and urgency'. In that case, the bank failed to
    update its fraud engine following a warning from Mastercard the previous day
    of a new type of card scam.

* Chapter 13 - Locks and Alarms

  Most security engineers nowadays focus on electronic systems, but physical
  protection cannot be neglected.

  First, if you're advising on a company's overall risk management, then walls
  and locks are a factor.

  Second, as it's easier to teach someone with an electrical engineering or
  computer science background the basics of physical security than the other way
  round, interactions between physical and logical protection are usually up to
  the systems person to manage.

  Third, you will often be asked for your opinion on your client's
  installations - which may have been built by contractors with little
  understanding of system issues. You'll need to be able to give informed, but
  diplomatic, advice.

  Fourth, many information security mechanisms can be defeated if a bad man gets
  physical access, whether at the factory, or during shipment, or before
  installation.

  Fifth, many mechanical locks have recently been completely compromised by
  'bumping', an easy covert-entry technique; their manufactures often seem
  unaware of vulnerabilities that enable their products to be quickly bypassed.

  Finally, many of the electronic locks that are replacing them are easy to
  compromise, either because they use cryptography that's broken (such as Mifare
  classic) or because of poor integration of the mechanical and digital
  components.

** Threats and barriers

   [...] The design and testing of entry controls and alarms are driven by a
   policy based on:

   + Deter - detect - alarm - delay - respond

*** Deterrence

    The first consideration is whether you can prevent bad people from ever
    trying to break in. In this regard, it's a good idea to make your asset
    anonymous and inconspicuous if you can.

    Location matters; some neighbourhoods have much less crime than others. Part
    of this has to do with whether property nearby is protected, and how easy it
    is for a crook to tell which properties are protected.

    If owners just install visible alarms, they may redistribute crime to their
    neighbours; but invisible alarms that get criminals caught rather than just
    sent next door can deter crime in a whole neighbourhood.

    For example, Ian Ayres and Steven Levitt studied the effect on auto thefts
    of Lojack, a radio tag that's embedded invisibly in cars and lets the police
    find them if they're stolen. In tows where a lot of cars have Lojack, car
    thieves are caught quickly, and 'chop-shops' that break up stole cars for
    parts are closed down. [...] The same applies to real estate; a
    neighbourhood in which lots of houses have high-grade alarms that quietly
    call the police is a dangerous place for a burglar to work.

**** Situational crime prevention

     + Increase the risks and efforts
     + Reduce the rewards and provocations
     + Remove excuses

** Alarms

   Alarms are used to deal with much more than burglary. Their applications
   range from monitoring freezer temperatures in supermarkets (so staff don't
   'accidentally' switch off freezer cabinets in the hope of being given food to
   take home), right through to improvised explosive devices in conflict zones
   that are often booby-trapped.

   [...]

   The /Titanic Effect/ of over-reliance on the latest technology often blinds
   people to common sense.

* Chapter 14 - Monitoring and Metering

  No notes for this chapter.
